# 概率统计学习资料

---

# 第一部分：随机事件与概率

---

## 第1章 随机现象与样本空间

### 1.1 什么是随机现象

世界上的现象分两类。

**确定性现象**：条件完全决定结果。把水加热到100度，它一定沸腾。松手，物体一定下落。

**随机现象**：条件相同，结果不同。抛一枚硬币，可能正面也可能反面。同一个人投篮，可能进也可能不进。

随机现象有两个特点：
- 每次试验前，无法确定会出现哪个结果
- 所有可能的结果是已知的

概率论研究的就是随机现象中的规律性。虽然单次结果不确定，但大量重复后，结果的比例会趋于稳定。

---

### 1.2 样本空间与样本点

**样本空间**就是一次试验所有可能结果的集合，记作Ω（读作omega）或S。

**样本点**是样本空间中的每一个元素，就是每一个可能的结果。

**例1：抛一枚硬币**

样本空间 Ω = {正面, 反面}

读作：样本空间等于集合，包含正面和反面两个元素。

这里有2个样本点。

**例2：抛两枚硬币**

样本空间 Ω = {(正,正), (正,反), (反,正), (反,反)}

读作：样本空间等于集合，包含四个元素，分别是两个都正、第一个正第二个反、第一个反第二个正、两个都反。

这里有4个样本点。

**例3：掷一颗骰子**

样本空间 Ω = {1, 2, 3, 4, 5, 6}

这里有6个样本点。

**例4：测量灯泡寿命**

样本空间 Ω = {t : t ≥ 0}

读作：样本空间等于所有大于等于零的实数t组成的集合。

这是一个无穷样本空间。

**关键点**：样本空间不唯一，取决于你关心什么。

抛两枚硬币，如果你只关心正面朝上的个数，样本空间可以写成 Ω = {0, 1, 2}。

---

### 1.3 事件的表示

**事件**是样本空间的子集，就是某些样本点的集合。

用大写字母A、B、C表示事件。

**例：掷骰子**

样本空间 Ω = {1, 2, 3, 4, 5, 6}

- 事件A = "出现偶数" = {2, 4, 6}
- 事件B = "点数大于4" = {5, 6}
- 事件C = "点数小于3" = {1, 2}

**两个特殊事件**：

**必然事件**：每次试验一定发生，就是样本空间本身Ω。掷骰子，"点数在1到6之间"是必然事件。

**不可能事件**：每次试验一定不发生，记作∅（空集）。掷骰子，"点数等于7"是不可能事件。

**事件发生的定义**：如果试验的结果（样本点）落在事件A中，就说事件A发生了。

掷骰子掷出4，则事件A="出现偶数"发生了，因为4属于集合{2, 4, 6}。

---

### 1.4 事件的运算

事件是集合，所以可以用集合运算来描述事件之间的关系。

#### 1.4.1 并事件（或）

A∪B 读作"A并B"，表示A发生或B发生或两者都发生。

用集合语言说：属于A或属于B的所有样本点。

**例**：掷骰子，A = {2, 4, 6}，B = {5, 6}

A∪B = {2, 4, 5, 6}

读作：A并B等于集合2、4、5、6。

这表示"出现偶数或点数大于4"。

#### 1.4.2 交事件（且）

A∩B 读作"A交B"，表示A发生且B发生，两者同时发生。

用集合语言说：既属于A又属于B的所有样本点。

**例**：掷骰子，A = {2, 4, 6}，B = {5, 6}

A∩B = {6}

读作：A交B等于集合，只包含元素6。

这表示"出现偶数且点数大于4"，只有6满足。

简写：A∩B 常写成 AB。

#### 1.4.3 补事件（非）

Ā 读作"A的补"或"A拔"，表示A不发生。

用集合语言说：样本空间中不属于A的所有样本点。

**例**：掷骰子，A = {2, 4, 6}

Ā = {1, 3, 5}

读作：A的补等于集合1、3、5。

这表示"没有出现偶数"，也就是"出现奇数"。

**重要性质**：

A ∪ Ā = Ω，读作：A并上A的补等于样本空间。事件和它的补一定有一个发生。

A ∩ Ā = ∅，读作：A交上A的补等于空集。事件和它的补不可能同时发生。

#### 1.4.4 差事件

A - B 读作"A减B"，表示A发生但B不发生。

用集合语言说：属于A但不属于B的样本点。

**例**：掷骰子，A = {2, 4, 6}，B = {5, 6}

A - B = {2, 4}

读作：A减B等于集合2、4。

这表示"出现偶数但点数不大于4"。

**等价写法**：A - B = A ∩ B̄，读作A减B等于A交上B的补。

#### 1.4.5 互斥事件

如果 A∩B = ∅，称A和B互斥，也叫互不相容。

意思是A和B不可能同时发生。

**例**：掷骰子，A = {1, 2}，B = {5, 6}

A∩B = ∅，所以A和B互斥。

"点数小于3"和"点数大于4"不可能同时发生。

#### 1.4.6 对立事件

如果 A∩B = ∅ 且 A∪B = Ω，称A和B对立。

意思是A和B不能同时发生，但必有一个发生。

对立事件一定互斥，但互斥事件不一定对立。

**例**：掷骰子

A = {1, 2, 3}，B = {4, 5, 6}

A和B互斥，且A∪B = Ω，所以A和B对立。

B就是Ā。

**区分互斥与对立**：

互斥只要求"不能同时发生"。

对立还要求"必有一个发生"。

**例**：A = {1, 2}，C = {3, 4}

A和C互斥，但不对立，因为A∪C = {1, 2, 3, 4} ≠ Ω。

掷出5或6时，A和C都没发生。

---

### 1.5 本章公式速查

| 运算 | 符号 | 读法 | 含义 |
|-----|------|------|-----|
| 并 | A∪B | A并B | A或B发生 |
| 交 | A∩B 或 AB | A交B | A且B发生 |
| 补 | Ā | A拔 | A不发生 |
| 差 | A-B | A减B | A发生B不发生 |
| 互斥 | A∩B=∅ | A交B等于空集 | A、B不能同时发生 |
| 对立 | A∩B=∅ 且 A∪B=Ω | — | A、B恰有一个发生 |

---

### 1.6 德摩根律

这是集合运算中最重要的公式之一。

**第一条**：

$\overline{A \cup B} = \bar{A} \cap \bar{B}$

读作：A并B的补，等于A的补交上B的补。

直觉：A或B没发生，等价于A没发生且B没发生。

**第二条**：

$\overline{A \cap B} = \bar{A} \cup \bar{B}$

读作：A交B的补，等于A的补并上B的补。

直觉：A且B没发生，等价于A没发生或B没发生（至少一个没发生）。

**推广到n个事件**：

$\overline{\bigcup_{i=1}^{n} A_i} = \bigcap_{i=1}^{n} \bar{A_i}$

读作：n个事件并集的补，等于各事件补集的交集。

直觉："至少一个发生"没发生，等于"全都没发生"。

$\overline{\bigcap_{i=1}^{n} A_i} = \bigcup_{i=1}^{n} \bar{A_i}$

读作：n个事件交集的补，等于各事件补集的并集。

直觉："全都发生"没发生，等于"至少一个没发生"。

---

### 本章小结

1. **随机现象**：条件相同结果不同，但结果有规律
2. **样本空间Ω**：所有可能结果的集合
3. **事件**：样本空间的子集
4. **事件运算**：并（或）、交（且）、补（非）、差
5. **互斥**：不能同时发生
6. **对立**：不能同时发生 + 必有一个发生
7. **德摩根律**：补集与并交的转换规则

---

*下一章：概率的定义与性质*

---

## 第2章 概率的定义与性质

### 2.1 概率的古典定义

**古典概型**需要满足两个条件：
- 样本空间有限：只有有限个样本点
- 等可能性：每个样本点出现的可能性相同

在古典概型下，事件A的概率定义为：

P(A) = A包含的样本点数 / 样本空间的样本点总数 = m / n

读作：A的概率等于m除以n，其中m是A包含的样本点数，n是总样本点数。

**例1：掷一颗均匀骰子，求出现偶数的概率**

样本空间 Ω = {1, 2, 3, 4, 5, 6}，共6个样本点。

事件A = "出现偶数" = {2, 4, 6}，共3个样本点。

P(A) = 3/6 = 1/2

读作：出现偶数的概率等于二分之一。

**例2：同时抛两枚均匀硬币，求至少一枚正面的概率**

样本空间 Ω = {(正,正), (正,反), (反,正), (反,反)}，共4个样本点。

事件A = "至少一枚正面" = {(正,正), (正,反), (反,正)}，共3个样本点。

P(A) = 3/4

读作：至少一枚正面的概率等于四分之三。

**古典定义的局限**：
- 样本空间必须有限
- 必须等可能，但"等可能"本身就是概率概念，有循环定义嫌疑
- 无法处理无穷样本空间

---

### 2.2 概率的统计定义

**频率**：做n次试验，事件A发生了m次，则A的频率为 m/n。

记作：f_n(A) = m/n

读作：A在n次试验中的频率等于m除以n。

**频率的稳定性**：当试验次数n足够大时，频率会稳定在某个值附近波动。

**统计定义**：这个稳定值就是概率。

P(A) = lim(n→∞) f_n(A)

读作：A的概率等于当试验次数趋向无穷时，A的频率的极限值。

**例：抛硬币**

抛10次，可能6正4反，频率0.6。

抛100次，可能48正52反，频率0.48。

抛10000次，频率通常在0.49到0.51之间。

次数越多，频率越接近0.5。

**统计定义的问题**：
- 无法真正做无穷次试验
- 只是描述性的，不是严格数学定义
- 但它揭示了概率的本质含义：长期频率

---

### 2.3 概率的公理化定义

柯尔莫哥洛夫在1933年给出了概率的公理化定义，用三条公理完全确定概率的数学性质。

设Ω是样本空间，P是定义在事件上的函数，满足：

**公理1（非负性）**：对任意事件A，P(A) ≥ 0

读作：任何事件的概率都大于等于零。

**公理2（规范性）**：P(Ω) = 1

读作：样本空间的概率等于1，即必然事件的概率是1。

**公理3（可列可加性）**：若A₁, A₂, A₃, ... 两两互斥，则

P(A₁ ∪ A₂ ∪ A₃ ∪ ...) = P(A₁) + P(A₂) + P(A₃) + ...

读作：互斥事件并集的概率，等于各事件概率之和。

这三条公理是概率论的基石。所有概率的性质都可以从这三条推出。

---

### 2.4 概率的基本性质

以下性质全部由三条公理推导而来。

**性质1：不可能事件的概率为0**

P(∅) = 0

读作：空集的概率等于零。

推导思路：Ω = Ω ∪ ∅ ∪ ∅ ∪ ...，由可加性和规范性可得。

**性质2：有限可加性**

若A₁, A₂, ..., Aₙ 两两互斥，则

P(A₁ ∪ A₂ ∪ ... ∪ Aₙ) = P(A₁) + P(A₂) + ... + P(Aₙ)

读作：有限个互斥事件并集的概率，等于各概率之和。

**性质3：补事件公式**

P(Ā) = 1 - P(A)

读作：A补的概率等于1减去A的概率。

推导：A ∪ Ā = Ω，且A与Ā互斥，所以 P(A) + P(Ā) = 1。

这是最常用的公式之一。求"至少一个"的概率时，常用 1 减去"一个都没有"的概率。

**性质4：单调性**

若 A ⊂ B，则 P(A) ≤ P(B)

读作：如果A包含于B，则A的概率小于等于B的概率。

直觉：A发生必然导致B发生，所以B发生的可能性不低于A。

**性质5：概率的范围**

0 ≤ P(A) ≤ 1

读作：任何事件的概率都在0和1之间。

由非负性、规范性和单调性可得。

**性质6：加法公式（一般形式）**

P(A ∪ B) = P(A) + P(B) - P(AB)

读作：A并B的概率，等于A的概率加B的概率，再减去AB的概率。

为什么要减？因为AB被算了两次。

**例**：掷骰子，A = "偶数"，B = "大于3"

P(A) = 3/6 = 1/2

P(B) = 3/6 = 1/2

AB = {4, 6}，P(AB) = 2/6 = 1/3

P(A∪B) = 1/2 + 1/2 - 1/3 = 2/3

验证：A∪B = {2, 4, 5, 6}，确实是4个点，4/6 = 2/3。

**特殊情况**：若A、B互斥，则P(AB) = 0，加法公式退化为

P(A ∪ B) = P(A) + P(B)

---

### 2.5 加法公式的推广

**三个事件的加法公式**：

P(A∪B∪C) = P(A) + P(B) + P(C) - P(AB) - P(AC) - P(BC) + P(ABC)

读作：三个事件并集的概率，等于各自概率之和，减去两两交的概率，再加上三个都交的概率。

记忆口诀：加单减双加三。

**n个事件的加法公式（容斥原理）**：

P(∪Aᵢ) = ΣP(Aᵢ) - ΣP(AᵢAⱼ) + ΣP(AᵢAⱼAₖ) - ... + (-1)ⁿ⁺¹P(A₁A₂...Aₙ)

读作：n个事件并集的概率，等于单个概率之和，减去两两交的概率之和，加上三三交的概率之和，如此交替加减，最后一项的符号由n决定。

---

### 2.6 古典概型的计算技巧

古典概型的核心是**计数**。计数的两大工具：排列和组合。

**排列数**：从n个不同元素中取r个，按顺序排列。

A(n,r) = n! / (n-r)! = n × (n-1) × ... × (n-r+1)

读作：A n取r，等于n的阶乘除以n减r的阶乘。

**例**：5个人排成一排，有 A(5,5) = 5! = 120 种排法。

**组合数**：从n个不同元素中取r个，不考虑顺序。

C(n,r) = n! / [r!(n-r)!] = A(n,r) / r!

读作：C n取r，等于n的阶乘除以r的阶乘乘以n减r的阶乘。

也写作 (n choose r)，读作"从n个里选r个"。

**例**：5个人选2人组队，有 C(5,2) = 10 种选法。

**核心区别**：排列考虑顺序，组合不考虑顺序。

AB和BA在排列中是两种，在组合中是一种。

---

### 2.7 古典概型例题

**例1：生日问题**

n个人中，至少两人生日相同的概率是多少？假设每人生日等可能落在365天中的任一天。

设事件A = "至少两人生日相同"

直接算A很复杂。用补事件：Ā = "所有人生日都不同"

P(Ā) = 365 × 364 × ... × (365-n+1) / 365ⁿ

读作：所有人生日都不同的概率，等于365乘364乘到365减n加1，除以365的n次方。

所以 P(A) = 1 - P(Ā)

代入n=23，P(A) ≈ 0.507，超过一半！

23人的班级里，有超过50%的概率存在同一天生日的两人。这个结果违反直觉，但数学不会骗人。

**例2：抽签问题**

袋中有5个球，3白2黑。不放回抽2个，求两个都是白球的概率。

方法一：按顺序考虑

第一次抽白：3/5

第二次抽白（已抽走一个白）：2/4

P = 3/5 × 2/4 = 6/20 = 3/10

方法二：用组合

总共选法：C(5,2) = 10

两白的选法：C(3,2) = 3

P = 3/10

两种方法结果相同。

**例3：抽签公平性**

n张签中有1张中奖签。每人依次抽一张，不放回。先抽和后抽，中奖概率相同吗？

设Aᵢ = "第i个人中奖"

P(A₁) = 1/n，这是显然的。

P(A₂) = ?

第二个人中奖，需要第一个人没中。

P(A₂) = P(第一人没中) × P(第一人没中的条件下第二人中)
      = (n-1)/n × 1/(n-1) = 1/n

同理可证 P(Aᵢ) = 1/n 对所有i成立。

结论：抽签与顺序无关，先抽后抽机会均等。

---

### 本章小结

1. **古典定义**：P(A) = m/n，要求有限等可能
2. **统计定义**：概率是频率的极限，揭示本质含义
3. **公理化定义**：非负性、规范性、可列可加性
4. **补事件公式**：P(Ā) = 1 - P(A)，求"至少"常用
5. **加法公式**：P(A∪B) = P(A) + P(B) - P(AB)
6. **排列组合**：排列有序A(n,r)，组合无序C(n,r)
7. **抽签公平**：顺序不影响概率

---

*下一章：条件概率与独立性*

---

## 第3章 条件概率与独立性

### 3.1 条件概率的直觉

**问题**：已知某些信息后，事件的概率会变吗？

**例**：掷一颗骰子。

原本，P(点数=6) = 1/6。

现在告诉你：点数大于3。

在这个信息下，P(点数=6) 变成多少？

已知点数大于3，可能的结果只有 {4, 5, 6}，共3个。

其中点数=6只有1个。

所以新的概率是 1/3。

这就是**条件概率**：在已知某事件发生的条件下，另一事件发生的概率。

---

### 3.2 条件概率的定义

设A、B是两个事件，P(B) > 0。在事件B发生的条件下，事件A发生的条件概率定义为：

P(A|B) = P(AB) / P(B)

读作：A在B条件下的概率，等于AB的概率除以B的概率。

竖线"|"读作"在...条件下"或"给定"。

**直觉理解**：

B发生后，样本空间从Ω缩小为B。

A在新样本空间中的部分是AB。

新的概率 = AB占B的比例 = P(AB)/P(B)。

**例**：掷骰子，A = {6}，B = {4,5,6}

P(A) = 1/6，P(B) = 3/6 = 1/2，P(AB) = P({6}) = 1/6

P(A|B) = P(AB)/P(B) = (1/6)/(1/2) = 1/3

与直觉一致。

---

### 3.3 乘法公式

把条件概率的定义式变形：

P(AB) = P(B) × P(A|B)

读作：AB的概率，等于B的概率乘以A在B条件下的概率。

也可以写成：

P(AB) = P(A) × P(B|A)

读作：AB的概率，等于A的概率乘以B在A条件下的概率。

这就是**乘法公式**，用于计算联合概率。

**推广到多个事件**：

P(ABC) = P(A) × P(B|A) × P(C|AB)

读作：ABC的概率，等于A的概率，乘以B在A条件下的概率，再乘以C在AB条件下的概率。

**例**：袋中3白2黑球，不放回抽两次，求两次都是白球的概率。

设A₁ = "第一次白"，A₂ = "第二次白"

P(A₁) = 3/5

P(A₂|A₁) = 2/4 = 1/2（第一次抽走一个白球后，剩2白2黑）

P(A₁A₂) = P(A₁) × P(A₂|A₁) = 3/5 × 1/2 = 3/10

---

### 3.4 全概率公式

**场景**：事件A可能通过多种"原因"发生，每种原因的概率已知，求A的总概率。

**完备事件组**：设B₁, B₂, ..., Bₙ 满足：
- 两两互斥：Bᵢ ∩ Bⱼ = ∅（i≠j）
- 并集为全集：B₁ ∪ B₂ ∪ ... ∪ Bₙ = Ω

则称 {B₁, B₂, ..., Bₙ} 为样本空间的一个**划分**或**完备事件组**。

**全概率公式**：

P(A) = P(B₁)P(A|B₁) + P(B₂)P(A|B₂) + ... + P(Bₙ)P(A|Bₙ)

读作：A的概率，等于各种原因的概率乘以在该原因下A发生的概率，然后全部加起来。

简写：P(A) = Σ P(Bᵢ)P(A|Bᵢ)

**直觉**：分情况讨论，然后汇总。

**例：产品检验**

工厂有两条生产线。

甲线产量占60%，次品率2%。

乙线产量占40%，次品率5%。

随机取一件产品，求它是次品的概率。

设A = "次品"，B₁ = "来自甲线"，B₂ = "来自乙线"

P(B₁) = 0.6，P(B₂) = 0.4

P(A|B₁) = 0.02，P(A|B₂) = 0.05

P(A) = P(B₁)P(A|B₁) + P(B₂)P(A|B₂)
     = 0.6 × 0.02 + 0.4 × 0.05
     = 0.012 + 0.020
     = 0.032

次品率是3.2%。

---

### 3.5 贝叶斯公式

**全概率公式的思维**：由原因推结果。已知各原因的概率，求结果的概率。

**贝叶斯公式的思维**：由结果反推原因。已知结果发生了，求各原因的概率。

**贝叶斯公式**：

P(Bⱼ|A) = P(Bⱼ)P(A|Bⱼ) / P(A)
        = P(Bⱼ)P(A|Bⱼ) / Σ P(Bᵢ)P(A|Bᵢ)

读作：在A发生的条件下Bⱼ的概率，等于Bⱼ的概率乘以Bⱼ条件下A的概率，再除以A的总概率。

**术语**：
- P(Bⱼ)：**先验概率**，观测前对原因的判断
- P(Bⱼ|A)：**后验概率**，观测后对原因的判断
- P(A|Bⱼ)：**似然**，该原因产生该结果的可能性

**贝叶斯公式的本质**：用观测到的结果更新对原因的判断。

**例：继续上面的产品检验**

已知取到的是次品，求它来自甲线的概率。

P(B₁|A) = P(B₁)P(A|B₁) / P(A)
        = (0.6 × 0.02) / 0.032
        = 0.012 / 0.032
        = 0.375

虽然甲线产量占60%，但次品来自甲线的概率只有37.5%。

因为乙线次品率更高，次品更可能来自乙线。

**例：医学检测**

某疾病在人群中患病率为1%。

检测方法：患者检测阳性的概率99%（灵敏度），健康人检测阴性的概率95%（特异度）。

一个人检测阳性，他真正患病的概率是多少？

设A = "检测阳性"，B = "患病"

P(B) = 0.01（先验：患病率）

P(B̄) = 0.99

P(A|B) = 0.99（患者检出阳性）

P(A|B̄) = 0.05（健康人误检为阳性）

先算P(A)：

P(A) = P(B)P(A|B) + P(B̄)P(A|B̄)
     = 0.01 × 0.99 + 0.99 × 0.05
     = 0.0099 + 0.0495
     = 0.0594

再算P(B|A)：

P(B|A) = P(B)P(A|B) / P(A)
       = 0.0099 / 0.0594
       ≈ 0.167

检测阳性，实际患病概率只有约16.7%！

**为什么这么低？**

因为患病率只有1%，而健康人误检阳性有5%。

基数效应：99个健康人中约5个误检阳性，1个患者中约1个真阳性。

阳性人群中，真患者只占 1/(1+5) ≈ 16.7%。

这个例子说明：理解贝叶斯公式对正确解读检测结果至关重要。

---

### 3.5.1 深入理解：信息的完整性

**核心洞察**：条件概率P(A|B)中的B，不仅是你看到的"结果"，还包含产生这个结果的"机制"。

用一个经典问题来说明：**蒙提霍尔问题**。

**问题设定**：

三张牌扣着，一张黑桃（奖品），两张红桃。

你选了一张。

裁判翻开剩下两张中的一张，是红桃。

问：要不要换牌？

**关键在于：裁判是怎么翻牌的？**

**情况A：裁判是"故意的"（蒙提霍尔规则）**

裁判知道哪张是黑桃，他故意避开黑桃，只翻红桃。

你手里是黑桃的概率：1/3

策略：必须换，赢面2/3。

**情况B：裁判是"瞎蒙的"**

裁判随机翻一张，碰巧是红桃。

你手里是黑桃的概率：1/2

策略：换不换都一样，赢面50%。

**同样看到一张红桃，概率不同！**

区别在于"机制"：裁判脑子里怎么想的。

**当你不知道裁判意图时——模型不确定性**

假设你认为裁判有50%可能是故意的，50%可能是瞎蒙的。

这引入了**第二层不确定性**（Model Uncertainty）。

经过加权计算，你手里是黑桃的概率约为3/7（约43%），落在1/3和1/2之间。

**决策分析：支配性策略**

即使不知道裁判意图，最优决策仍然确定。

| 真实情况 | 不换的胜率 | 换牌的胜率 |
|---------|-----------|-----------|
| 裁判故意 | 1/3 (33%) | 2/3 (67%) |
| 裁判瞎蒙 | 1/2 (50%) | 1/2 (50%) |
| 综合 | ≤50% | ≥50% |

无论裁判怎么想，换牌的收益总是大于等于不换。

这叫**弱支配策略**：在所有情况下都不会更差，在某些情况下更好。

**数学启示**

P(A|B)中的B是一个完整的**上下文**，包含：
- **数据**：你看到的结果（一张红桃）
- **元数据**：产生这个结果的机制（裁判的规则）

只看数据不看机制，会导致严重错误：

**幸存者偏差**：二战统计飞机弹孔位置，只统计了飞回来的飞机。真正致命的部位没有弹孔——因为被击中那里的飞机都坠毁了。

**选择性报告**：论文显示p<0.05，但作者可能尝试了100种检验方法，只报告了显著的那个。

**做概率题和数据分析时，多问一句**：

这个数据是随机采样得到的，还是经过某种筛选机制后呈现给我的？

这往往是最关键的一步。

---

### 3.6 事件的独立性

**定义**：如果 P(AB) = P(A)P(B)，称事件A和B**独立**。

读作：如果AB的概率等于A的概率乘以B的概率，则A和B独立。

**等价条件**（任一成立即独立）：
- P(AB) = P(A)P(B)
- P(A|B) = P(A)
- P(B|A) = P(B)

**直觉**：B发生与否，不改变A的概率。两者互不影响。

**例**：抛两枚硬币

A = "第一枚正面"，B = "第二枚正面"

P(A) = 1/2，P(B) = 1/2

P(AB) = P(两枚都正面) = 1/4 = 1/2 × 1/2 = P(A)P(B)

所以A和B独立。

**注意**：独立与互斥是两回事！

互斥：A和B不能同时发生，P(AB) = 0

独立：A和B同时发生的概率等于各自概率之积

如果A、B都有正概率且互斥，则P(AB)=0 ≠ P(A)P(B)，所以**互斥的事件一定不独立**（除非某个概率为0）。

---

### 3.7 多个事件的独立性

**两两独立**：任意两个事件独立。

**相互独立**：任意k个事件的交的概率，等于各概率之积（k=2,3,...,n）。

相互独立比两两独立更强。

**三个事件相互独立的条件**：

需要同时满足以下四个等式：
- P(AB) = P(A)P(B)
- P(AC) = P(A)P(C)
- P(BC) = P(B)P(C)
- P(ABC) = P(A)P(B)P(C)

只满足前三个是两两独立，不一定相互独立。

**独立性的应用：系统可靠性**

n个元件串联，每个正常工作的概率为p，且相互独立。

系统正常 = 所有元件都正常

P(系统正常) = pⁿ

n个元件并联，每个正常工作的概率为p，且相互独立。

系统正常 = 至少一个元件正常

P(系统正常) = 1 - P(全坏) = 1 - (1-p)ⁿ

**例**：某元件可靠性p=0.9，3个串联和3个并联的系统可靠性？

串联：0.9³ = 0.729

并联：1 - 0.1³ = 1 - 0.001 = 0.999

串联会降低可靠性，并联会提高可靠性。

---

### 3.8 独立重复试验与二项概率

**伯努利试验**：只有两个结果的试验，成功（概率p）或失败（概率1-p）。

**n重伯努利试验**：把伯努利试验独立重复n次。

设X = n次试验中成功的次数

P(X = k) = C(n,k) × pᵏ × (1-p)ⁿ⁻ᵏ

读作：n次试验中恰好成功k次的概率，等于组合数C(n,k)，乘以p的k次方，乘以1减p的n减k次方。

**公式解释**：
- C(n,k)：从n次中选k次成功的方式数
- pᵏ：k次成功的概率
- (1-p)ⁿ⁻ᵏ：n-k次失败的概率

**例**：抛5次硬币，恰好3次正面的概率？

n=5，k=3，p=0.5

P(X=3) = C(5,3) × 0.5³ × 0.5²
       = 10 × 0.125 × 0.25
       = 0.3125

---

### 本章小结

1. **条件概率**：P(A|B) = P(AB)/P(B)，B发生后A的概率
2. **乘法公式**：P(AB) = P(A)P(B|A)，求联合概率
3. **全概率公式**：由原因推结果，分情况汇总
4. **贝叶斯公式**：由结果反推原因，更新判断
5. **先验→后验**：观测使我们对原因的判断更精确
6. **独立性**：P(AB) = P(A)P(B)，互不影响
7. **独立≠互斥**：互斥且正概率的事件一定不独立
8. **二项概率**：n次独立试验成功k次的概率

---

*下一章：随机变量的概念*

---

# 第二部分：随机变量及其分布

---

## 第4章 随机变量的概念

### 4.1 为什么需要随机变量

前三章用"事件"描述随机现象：A="出现偶数"，B="至少一个正面"。

问题：事件语言太具体，不便于数学运算。

解决：用**数值**代替事件。

掷骰子，不说"出现偶数"，直接说"点数X"，X可取1,2,3,4,5,6。

抛硬币3次，不说"至少两次正面"，直接说"正面次数Y"，Y可取0,1,2,3。

**随机变量**：把随机试验的结果用数值表示的变量。

记法：通常用大写字母 X, Y, Z 表示随机变量，用小写字母 x, y, z 表示它的取值。

---

### 4.2 随机变量的严格定义

设Ω是样本空间。**随机变量X**是定义在Ω上的实值函数：

X: Ω → R

读作：X是从样本空间Ω到实数集R的一个映射。

每个样本点ω对应一个实数X(ω)。

**例：掷两颗骰子**

样本空间 Ω = {(1,1), (1,2), ..., (6,6)}，共36个样本点。

定义 X = 两颗骰子点数之和。

X(1,1) = 2，X(1,2) = 3，...，X(6,6) = 12。

X把36个样本点映射到 {2, 3, 4, ..., 12} 这11个数值。

**本质**：随机变量是样本空间的"数值化"。

---

### 4.2.1 深入理解：X是函数，不是变量

**"随机变量"这个名字起得很糟糕。**

它听起来像代数里的x（未知数），但数学定义里它是一个**函数**。

**"翻译官"隐喻**

样本空间Ω是充满各种事物的"真实世界"：
- 抛硬币：Ω = {正面, 反面}
- 观察细胞：Ω = {活跃, 死亡, 分裂...}

问题：我们没法对"正面"、"反面"做加减乘除，也没法做微积分。数学家只喜欢处理数字。

X的作用就是"翻译官"：把真实世界的事件强行对应到实数。

- 输入：ω（样本点，比如"硬币是正面"）
- 规则X："如果是正面，记为1；如果是反面，记为0"
- 输出：1或0（实数）

公式 X(ω) = x 的含义：样本点ω经过X的翻译，变成了实数x。

**为什么需要这个定义？**

一旦把"正面"变成"1"，把"反面"变成"0"：
- 可以算期望（平均值）
- 可以算方差
- 可以画坐标轴
- 可以做微积分

随机变量X本质是一套**数字化规则**，把不确定的现实世界投影到实数轴上。

---

### 4.2.2 大写X与小写x的区别

这是概率论中最容易让人"晕车"的地方。大小写有严格规定，不能混用。

**大写X：函数本身（动态的、未知的）**

X代表"这次试验的结果"。在硬币落地之前，你不知道X等于几。

比喻：X是一个**盲盒**。

**小写x：实数参数（静态的、已知的）**

x只是实数轴上的一个普通数字（比如0.5, 1, 100）。

比喻：x是写在纸上的**中奖号码**。

**结合起来看：P(X > x)**

当我们写 P(X > 1) 时：

翻译成人话："盲盒里摸出来的数(X)比1(x)更大的概率是多少？"

- X是动的（取决于上帝掷骰子）
- 1是静的（取决于你的提问）

**为什么要大写？**

如果把X写成小写x，公式变成P(x > x)——左边是随机的，右边是固定的，看起来一样其实不一样。

大写是为了**避嫌**，明确身份：我是随机变量，不是普通代数！

**符号简写的真相**

完整写法（函数视角）：

P({ω ∈ Ω | X(ω) > 0.5})

读作：在样本空间Ω里找到所有"经过X映射后数值大于0.5"的样本点，把它们的概率加起来。

简写法（变量视角）：

P(X > 0.5)

为什么简写？因为每次都写ω和X(ω)太麻烦。数学家假装X就是一个会变魔术的数，虽然本质是函数，但运算时把它当作能在实数轴上乱跑的变量。

---

### 4.3 离散型随机变量

**定义**：如果随机变量X只能取有限个或可数无穷个值，称为**离散型随机变量**。

可数无穷：能与自然数一一对应，如 {0, 1, 2, 3, ...}。

**例**：
- 掷骰子的点数：取值 {1, 2, 3, 4, 5, 6}，有限
- 抛硬币直到出现正面的次数：取值 {1, 2, 3, ...}，可数无穷

---

### 4.4 概率分布列

离散型随机变量X的**概率分布列**（简称分布列）：

P(X = xᵢ) = pᵢ，i = 1, 2, 3, ...

读作：X等于xᵢ的概率是pᵢ。

**性质**：
- pᵢ ≥ 0（非负性）
- Σpᵢ = 1（归一性）

**表格形式**：

| X | x₁ | x₂ | x₃ | ... |
|---|----|----|----|----|
| P | p₁ | p₂ | p₃ | ... |

**例：掷一颗骰子**

| X | 1 | 2 | 3 | 4 | 5 | 6 |
|---|---|---|---|---|---|---|
| P | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |

**例：抛两枚硬币，X=正面个数**

| X | 0 | 1 | 2 |
|---|---|---|---|
| P | 1/4 | 2/4 | 1/4 |

P(X=1) = 2/4，因为"一正一反"有两种情况：(正,反)和(反,正)。

---

### 4.5 分布函数

**分布函数**F(x)定义为：

F(x) = P(X ≤ x)

读作：F(x)等于X小于等于x的概率。

**性质**：

1. **单调不减**：若 x₁ < x₂，则 F(x₁) ≤ F(x₂)
2. **右连续**：F(x) = F(x+0)
3. **边界值**：F(-∞) = 0，F(+∞) = 1

**用分布函数计算区间概率**：

P(a < X ≤ b) = F(b) - F(a)

读作：X大于a且小于等于b的概率，等于F(b)减F(a)。

**例：离散型的分布函数**

掷骰子，X是点数。

F(0) = P(X ≤ 0) = 0

F(1) = P(X ≤ 1) = 1/6

F(2) = P(X ≤ 2) = 2/6

...

F(6) = P(X ≤ 6) = 1

F(7) = 1（X不可能超过6）

离散型分布函数是阶梯函数，在每个取值点跳跃。

---

### 4.6 连续型随机变量

**定义**：如果存在非负函数f(x)，使得分布函数可以表示为：

F(x) = ∫_{-∞}^{x} f(t) dt

读作：F(x)等于f(t)从负无穷到x的积分。

则称X为**连续型随机变量**，f(x)称为X的**概率密度函数**（简称密度函数或PDF）。

**性质**：

1. f(x) ≥ 0（非负性）
2. ∫_{-∞}^{+∞} f(x) dx = 1（归一性）
3. P(a < X ≤ b) = ∫_{a}^{b} f(x) dx
4. F'(x) = f(x)（在f连续的点）

**重要区别**：

离散型：P(X = xᵢ) > 0，单点有概率。

连续型：P(X = a) = 0，单点概率为零！

直觉：连续型有无穷多个可能取值，每个点分到的概率是"无穷小"。

所以对连续型随机变量，P(X = 5) = 0，但X可以取到5。

概率为零不等于不可能！

---

### 4.6.1 深入理解：概率密度f(x)与概率P的关系

很多人对这两者混淆。在离散情况下它们是一回事；在连续情况下，发生了**维度的质变**。

用物理中的**"质量"与"密度"**来类比。

**想象一根不均匀的铁丝**

设X代表一根长度为1米的铁丝上的位置（从0到1）。

这根铁丝的总质量是1克（对应总概率为1）。

但铁丝粗细不均匀：有的地方很粗（概率大），有的地方很细（概率小）。

**概率 = 质量**

问：X落在区间[0.1, 0.2]的概率是多少？

答：相当于问"这一段铁丝有多重？"——有具体数值，比如0.05克。

**概率密度 = 线密度**

问：在x=0.5这一点，铁丝的密度是多少？

答：这就是f(0.5)。密度描述的是"这里很粗，如果取一小段，它会有多重"。

**核心矛盾与联系**

**矛盾点（最易错）**：

单点质量为0：x=0.5这一个点有多重？答案是0。因为一个点没有长度。

这就是为什么连续型 P(X=x) = 0。

单点密度不为0：x=0.5这一点有多密？答案是f(0.5)。可以是很大的数，甚至大于1。

**联系点（微元法）**：

P(x < X < x + dx) ≈ f(x) × dx

读作：X落在x到x+dx这一小段的概率，约等于密度乘以长度。

左边是概率（质量），右边是密度×长度。

**直观结论**：

- 概率是**面积**（积分结果）
- 密度是曲线的**高度**
- 要得到概率，必须把密度f(x)在一段区间上积分

**随堂检测**

设概率密度函数 f(x) = 3x²（在0<x<1范围内，其他为0）。

问1：P(X=0.5) 是多少？

答：0。单点的概率（质量）为零。

问2：x=0.5处的"可能性密集程度"是多少？

答：f(0.5) = 3×0.5² = 0.75。这是密度，不是概率。

问3：P(X>1) 是多少？

答：0。因为定义域只在0到1之间，1外面没有"铁丝"。

问4：f(x)可以大于1吗？

答：可以！密度可以很大，只要积分起来总面积是1。密度本身没有上限。

比如f(x)=10（在0到0.1之间），积分面积=10×0.1=1，完全合法。

**支撑集（Support）**

随机变量的**支撑集**是指它可能取值的范围。

f(x)=3x²的支撑集是(0,1)。在这个范围外，密度为0，概率也为0。

不同分布有不同支撑集：
- 均匀分布U(a,b)：支撑集是[a,b]
- 指数分布Exp(λ)：支撑集是[0,+∞)
- 正态分布N(μ,σ²)：支撑集是(-∞,+∞)，无限延伸但两端极细

---

### 4.7 常见离散分布

#### 4.7.1 0-1分布（伯努利分布）

最简单的分布。X只取0或1。

P(X = 1) = p，P(X = 0) = 1-p = q

记作 X ~ B(1, p)，读作"X服从参数为1和p的伯努利分布"。

应用：单次试验的成功与否。

#### 4.7.2 二项分布

n次独立伯努利试验，成功次数X的分布。

P(X = k) = C(n,k) × p^k × (1-p)^(n-k)，k = 0,1,2,...,n

读作：n次试验成功k次的概率，等于组合数C(n,k)乘以p的k次方乘以1减p的n减k次方。

记作 X ~ B(n, p)，读作"X服从参数为n和p的二项分布"。

**例**：抛10次硬币，正面次数X ~ B(10, 0.5)

P(X = 5) = C(10,5) × 0.5^10 = 252 × (1/1024) ≈ 0.246

#### 4.7.3 泊松分布

描述稀有事件在固定时间/空间内发生次数。

P(X = k) = (λ^k / k!) × e^(-λ)，k = 0,1,2,...

读作：X等于k的概率，等于λ的k次方除以k的阶乘，再乘以e的负λ次方。

记作 X ~ P(λ) 或 X ~ Poisson(λ)。

λ是单位时间（空间）内事件平均发生次数。

**应用场景**：
- 一小时内到达银行的顾客数
- 一页书中的印刷错误数
- 一天内某路口的交通事故数
- 放射性物质单位时间内的衰变次数

**泊松分布的来源**：二项分布的极限。

当 n→∞，p→0，且 np→λ 时，B(n,p) → P(λ)。

即：大量独立试验，每次成功概率很小，成功总次数近似泊松分布。

#### 4.7.4 几何分布

独立重复伯努利试验，首次成功时的试验次数。

P(X = k) = (1-p)^(k-1) × p，k = 1,2,3,...

读作：第k次试验才首次成功的概率，等于1减p的k减1次方乘以p。

记作 X ~ Geom(p)。

**无记忆性**：P(X > m+n | X > m) = P(X > n)

读作：已知前m次失败，再等n次才成功的概率，等于从头开始等n次成功的概率。

直觉：几何分布"不记得"之前失败了多少次。每次重新开始。

---

### 4.8 常见连续分布

#### 4.8.1 均匀分布

在区间[a,b]上等可能取值。

密度函数：f(x) = 1/(b-a)，当 a ≤ x ≤ b；其他为0。

读作：密度函数在a到b之间等于b减a分之一，其他地方为零。

记作 X ~ U(a, b)。

分布函数：
- F(x) = 0，当 x < a
- F(x) = (x-a)/(b-a)，当 a ≤ x ≤ b
- F(x) = 1，当 x > b

**例**：公交车每10分钟一班，你随机到达车站，等待时间X ~ U(0, 10)。

P(X < 3) = 3/10 = 0.3

#### 4.8.2 指数分布

描述等待时间、寿命等。

密度函数：f(x) = λe^(-λx)，当 x ≥ 0；其他为0。

读作：密度函数等于λ乘以e的负λx次方（x大于等于零时）。

记作 X ~ Exp(λ)。

分布函数：F(x) = 1 - e^(-λx)，当 x ≥ 0。

**无记忆性**：P(X > s+t | X > s) = P(X > t)

与几何分布类似，指数分布也有无记忆性。

应用：灯泡已经用了1000小时，再用500小时的概率，等于新灯泡用500小时的概率。

**泊松过程与指数分布的关系**：

若事件按泊松过程发生（强度λ），则相邻两次事件的时间间隔服从Exp(λ)。

#### 4.8.3 正态分布

最重要的连续分布。

密度函数：f(x) = (1/(σ√(2π))) × e^(-(x-μ)²/(2σ²))

读作：密度函数等于1除以σ乘根号2π，再乘以e的负(x减μ)平方除以2σ平方次方。

记作 X ~ N(μ, σ²)。

μ是均值（峰值位置），σ²是方差（分散程度）。

**标准正态分布**：μ=0，σ=1，记作 Z ~ N(0, 1)。

标准正态分布的分布函数记作 Φ(x)。

**任意正态分布的标准化**：

若 X ~ N(μ, σ²)，则 Z = (X-μ)/σ ~ N(0, 1)。

读作：X减μ再除以σ，服从标准正态分布。

**计算概率**：

P(a < X ≤ b) = Φ((b-μ)/σ) - Φ((a-μ)/σ)

Φ(x)的值查表或用计算器。

**3σ原则**：

P(|X-μ| < σ) ≈ 0.683

P(|X-μ| < 2σ) ≈ 0.954

P(|X-μ| < 3σ) ≈ 0.997

数据落在均值3个标准差之外的概率不到0.3%。

---

### 本章小结

1. **随机变量**：随机试验结果的数值化
2. **离散型**：取有限或可数个值，用分布列描述
3. **连续型**：用密度函数和分布函数描述
4. **分布函数**：F(x) = P(X ≤ x)，累积概率
5. **连续型单点概率为零**：P(X=a)=0，但不代表不可能
6. **二项分布**：n次独立试验的成功次数
7. **泊松分布**：稀有事件的发生次数
8. **正态分布**：最重要的分布，有3σ原则

---

*下一章：随机变量的数字特征*

---

## 第5章 随机变量的数字特征

### 5.1 为什么需要数字特征

分布函数或密度函数完整描述了随机变量，但信息量太大。

实际中，我们常常只关心几个关键数字：
- 平均水平是多少？（期望）
- 波动有多大？（方差）
- 分布偏向哪边？（偏度）

这些数字叫做**数字特征**，用少量数值概括分布的主要性质。

---

### 5.2 数学期望（均值）

#### 5.2.1 离散型的期望

设离散型随机变量X的分布列为 P(X = xᵢ) = pᵢ，则X的**数学期望**定义为：

E(X) = Σ xᵢ × pᵢ = x₁p₁ + x₂p₂ + x₃p₃ + ...

读作：E(X)等于所有可能取值乘以对应概率的和。

**直觉**：加权平均。每个取值按发生概率加权。

**例：掷骰子**

| X | 1 | 2 | 3 | 4 | 5 | 6 |
|---|---|---|---|---|---|---|
| P | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |

E(X) = 1×(1/6) + 2×(1/6) + 3×(1/6) + 4×(1/6) + 5×(1/6) + 6×(1/6)
     = (1+2+3+4+5+6)/6
     = 21/6
     = 3.5

掷骰子的平均点数是3.5。虽然骰子不可能掷出3.5，但这是长期平均值。

#### 5.2.2 连续型的期望

设连续型随机变量X的密度函数为f(x)，则X的期望定义为：

E(X) = ∫_{-∞}^{+∞} x × f(x) dx

读作：E(X)等于x乘以f(x)从负无穷到正无穷的积分。

**直觉**：把离散求和换成连续积分。

**例：均匀分布U(0,1)**

f(x) = 1（0 ≤ x ≤ 1）

E(X) = ∫₀¹ x × 1 dx = [x²/2]₀¹ = 1/2

均匀分布在(0,1)上的期望是0.5，正好是中点。

#### 5.2.3 期望的本质：大数定律的预告

如果把随机试验重复n次，结果分别是X₁, X₂, ..., Xₙ。

样本均值 = (X₁ + X₂ + ... + Xₙ) / n

当n→∞时，样本均值趋向于E(X)。

这就是**大数定律**的核心内容（后面会详细讲）。

期望E(X)就是"长期平均值"的数学刻画。

---

### 5.3 期望的性质

**性质1：常数的期望是自身**

E(c) = c

读作：常数c的期望就是c。

**性质2：线性性质**

E(aX + b) = aE(X) + b

读作：aX加b的期望，等于a乘以X的期望再加b。

**性质3：加法性质（非常重要！）**

E(X + Y) = E(X) + E(Y)

读作：X加Y的期望，等于X的期望加Y的期望。

**注意**：这个性质**不要求X和Y独立**！无论X、Y是否相关，期望的加法都成立。

**性质4：独立时乘法成立**

若X、Y独立，则：

E(XY) = E(X) × E(Y)

读作：若X、Y独立，XY的期望等于X的期望乘以Y的期望。

**警告**：若X、Y不独立，E(XY) ≠ E(X)E(Y)！

---

### 5.4 常见分布的期望

| 分布 | 记号 | 期望E(X) |
|------|------|---------|
| 0-1分布 | B(1,p) | p |
| 二项分布 | B(n,p) | np |
| 泊松分布 | P(λ) | λ |
| 几何分布 | Geom(p) | 1/p |
| 均匀分布 | U(a,b) | (a+b)/2 |
| 指数分布 | Exp(λ) | 1/λ |
| 正态分布 | N(μ,σ²) | μ |

**记忆技巧**：

二项分布：n次试验，每次成功概率p，平均成功np次。

泊松分布：λ本身就是平均发生次数。

几何分布：成功概率p，平均要试1/p次才成功。

指数分布：平均等待时间是1/λ。

---

### 5.5 方差

#### 5.5.1 方差的定义

**方差**衡量随机变量偏离期望的程度。

定义：

Var(X) = E[(X - E(X))²]

读作：X的方差等于X减去X的期望的平方的期望。

也常记作 D(X) 或 σ²。

**计算公式（更实用）**：

Var(X) = E(X²) - [E(X)]²

读作：方差等于X平方的期望减去X期望的平方。

**直觉**：方差是"平均的偏差平方"。偏差越大，方差越大。

#### 5.5.2 标准差

**标准差**是方差的平方根：

σ = √Var(X)

读作：标准差sigma等于方差的平方根。

标准差与X有相同的量纲（单位），更便于解释。

**例**：身高的方差单位是"厘米²"，标准差单位是"厘米"。

#### 5.5.3 方差的直觉

方差=0：X总是等于期望，没有随机性。

方差小：X的取值集中在期望附近。

方差大：X的取值分散，远离期望的可能性大。

---

### 5.6 方差的性质

**性质1：常数的方差是0**

Var(c) = 0

常数没有波动。

**性质2：线性变换**

Var(aX + b) = a²Var(X)

读作：aX加b的方差，等于a平方乘以X的方差。

**注意**：加常数b不改变方差（只是平移），乘以a使方差变为a²倍。

**性质3：独立时方差可加**

若X、Y独立，则：

Var(X + Y) = Var(X) + Var(Y)

读作：若X、Y独立，X加Y的方差等于两者方差之和。

**警告**：若X、Y不独立，这个公式不成立！

**推广**：若X₁, X₂, ..., Xₙ相互独立，则：

Var(X₁ + X₂ + ... + Xₙ) = Var(X₁) + Var(X₂) + ... + Var(Xₙ)

---

### 5.7 常见分布的方差

| 分布 | 记号 | 方差Var(X) |
|------|------|-----------|
| 0-1分布 | B(1,p) | p(1-p) |
| 二项分布 | B(n,p) | np(1-p) |
| 泊松分布 | P(λ) | λ |
| 几何分布 | Geom(p) | (1-p)/p² |
| 均匀分布 | U(a,b) | (b-a)²/12 |
| 指数分布 | Exp(λ) | 1/λ² |
| 正态分布 | N(μ,σ²) | σ² |

**记忆技巧**：

泊松分布：期望和方差都是λ！这是泊松分布的特征。

正态分布：参数σ²本身就是方差。

二项分布：方差 = np(1-p)，当p=0.5时方差最大。

---

### 5.8 切比雪夫不等式

这是一个非常有用的不等式，只需要知道期望和方差，就能估计概率。

**切比雪夫不等式**：

P(|X - μ| ≥ kσ) ≤ 1/k²

读作：X偏离均值μ超过k个标准差的概率，不超过1除以k平方。

等价形式：

P(|X - μ| < kσ) ≥ 1 - 1/k²

读作：X落在均值k个标准差以内的概率，至少是1减1除以k平方。

**例**：

k=2：P(|X-μ| < 2σ) ≥ 1 - 1/4 = 0.75

k=3：P(|X-μ| < 3σ) ≥ 1 - 1/9 ≈ 0.889

**意义**：

这个不等式对**任何分布**都成立！只要方差存在。

不需要知道具体分布形式，就能给出概率的下界。

正态分布的3σ原则（99.7%）比切比雪夫下界（88.9%）更紧，说明正态分布比"一般分布"更集中。

---

### 5.9 协方差与相关系数

#### 5.9.1 协方差

衡量两个随机变量的**线性相关程度**。

定义：

Cov(X, Y) = E[(X - E(X))(Y - E(Y))]

读作：X和Y的协方差，等于X减均值乘以Y减均值的期望。

**计算公式**：

Cov(X, Y) = E(XY) - E(X)E(Y)

读作：协方差等于XY的期望减去X期望乘以Y期望。

**直觉**：

Cov > 0：X大时Y倾向于大，正相关。

Cov < 0：X大时Y倾向于小，负相关。

Cov = 0：X和Y不相关（但不一定独立！）。

**重要关系**：

Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)

当X、Y独立时，Cov(X,Y)=0，所以方差可加。

#### 5.9.2 相关系数

协方差受量纲影响。为了消除量纲，定义**相关系数**：

ρ(X, Y) = Cov(X, Y) / (σₓ × σᵧ)

读作：相关系数等于协方差除以两个标准差的乘积。

**性质**：

-1 ≤ ρ ≤ 1

|ρ| = 1：X和Y完全线性相关，Y = aX + b。

ρ = 0：X和Y不相关。

|ρ|接近1：强线性相关。

|ρ|接近0：弱线性相关或非线性相关。

**警告**：

不相关（ρ=0）不等于独立！

独立一定不相关，但不相关不一定独立。

不相关只是说没有**线性**关系，可能存在非线性关系。

**例**：设X ~ N(0,1)，Y = X²。

Y完全由X决定（强依赖），但 ρ(X, Y) = 0（不相关）！

因为Y和X的关系是非线性的。

---

### 本章小结

1. **期望E(X)**：加权平均，长期均值
2. **期望的线性性**：E(aX+b) = aE(X)+b，E(X+Y) = E(X)+E(Y)
3. **方差Var(X)**：偏离期望的程度
4. **方差公式**：Var(X) = E(X²) - [E(X)]²
5. **标准差σ**：方差的平方根，与X同量纲
6. **独立时方差可加**：Var(X+Y) = Var(X)+Var(Y)
7. **切比雪夫不等式**：P(|X-μ| ≥ kσ) ≤ 1/k²
8. **协方差**：衡量线性相关，Cov(X,Y) = E(XY) - E(X)E(Y)
9. **相关系数ρ**：标准化的协方差，-1到1之间
10. **不相关≠独立**：不相关只排除线性关系

---

*下一章：多维随机变量*

---

## 第6章 多维随机变量

### 6.1 为什么需要多维随机变量

现实中，我们常常需要**同时考虑多个随机变量**。

**例**：
- 一个人的身高X和体重Y
- 某地区的降雨量X和农作物产量Y
- 股票A的涨跌X和股票B的涨跌Y

单独研究X或Y不够，我们需要研究它们的**联合规律**。

---

### 6.2 二维离散型随机变量

#### 6.2.1 联合分布列

设(X, Y)是二维离散型随机变量，X取值为x₁, x₂, ...，Y取值为y₁, y₂, ...。

**联合分布列**定义为：

P(X = xᵢ, Y = yⱼ) = pᵢⱼ

读作：X等于xᵢ且Y等于yⱼ的概率，记为pᵢⱼ。

**表格形式**：

|  | Y=y₁ | Y=y₂ | Y=y₃ | ... |
|--|------|------|------|-----|
| X=x₁ | p₁₁ | p₁₂ | p₁₃ | ... |
| X=x₂ | p₂₁ | p₂₂ | p₂₃ | ... |
| ... | ... | ... | ... | ... |

**性质**：

1. pᵢⱼ ≥ 0（非负性）
2. ΣᵢΣⱼ pᵢⱼ = 1（归一性）

读作：所有pᵢⱼ加起来等于1。

**例：同时掷两枚骰子**

设X = 第一枚点数，Y = 第二枚点数。

P(X=i, Y=j) = 1/36，对所有i, j ∈ {1,2,3,4,5,6}。

这是**均匀联合分布**。

#### 6.2.2 边缘分布

已知联合分布，如何得到X单独的分布？

**边缘分布列**：

P(X = xᵢ) = Σⱼ pᵢⱼ = pᵢ₁ + pᵢ₂ + pᵢ₃ + ...

读作：X等于xᵢ的概率，等于对应行所有概率之和。

P(Y = yⱼ) = Σᵢ pᵢⱼ = p₁ⱼ + p₂ⱼ + p₃ⱼ + ...

读作：Y等于yⱼ的概率，等于对应列所有概率之和。

**直觉**："边缘"指的是在表格的边缘求和。

**例**：

|  | Y=0 | Y=1 | 边缘P(X) |
|--|-----|-----|---------|
| X=0 | 0.2 | 0.1 | **0.3** |
| X=1 | 0.3 | 0.4 | **0.7** |
| 边缘P(Y) | **0.5** | **0.5** | 1 |

P(X=0) = 0.2 + 0.1 = 0.3

P(Y=1) = 0.1 + 0.4 = 0.5

**重要**：知道边缘分布，**不能**反推联合分布！联合分布包含更多信息。

---

### 6.3 二维连续型随机变量

#### 6.3.1 联合密度函数

设(X, Y)是二维连续型随机变量，如果存在函数f(x, y)，使得对任意区域D：

P((X, Y) ∈ D) = ∬_D f(x, y) dx dy

读作：(X, Y)落在区域D内的概率，等于f(x, y)在D上的二重积分。

则f(x, y)称为**联合概率密度函数**。

**性质**：

1. f(x, y) ≥ 0（非负性）
2. ∬_{-∞}^{+∞} f(x, y) dx dy = 1（归一性）

读作：f(x, y)在整个平面上的积分等于1。

#### 6.3.2 边缘密度函数

已知联合密度f(x, y)，X的边缘密度为：

fₓ(x) = ∫_{-∞}^{+∞} f(x, y) dy

读作：X的边缘密度等于f(x, y)对y从负无穷到正无穷积分。

Y的边缘密度为：

fᵧ(y) = ∫_{-∞}^{+∞} f(x, y) dx

读作：Y的边缘密度等于f(x, y)对x从负无穷到正无穷积分。

**直觉**：把另一个变量"积掉"。

**例：均匀分布在单位正方形上**

f(x, y) = 1，当0 ≤ x ≤ 1且0 ≤ y ≤ 1；其他为0。

边缘密度：

fₓ(x) = ∫₀¹ 1 dy = 1，当0 ≤ x ≤ 1

fᵧ(y) = ∫₀¹ 1 dx = 1，当0 ≤ y ≤ 1

X和Y都是U(0,1)均匀分布。

---

### 6.4 条件分布

已知Y = y时，X的分布是什么？

#### 6.4.1 离散型的条件分布

P(X = xᵢ | Y = yⱼ) = P(X = xᵢ, Y = yⱼ) / P(Y = yⱼ) = pᵢⱼ / p·ⱼ

读作：在Y等于yⱼ的条件下，X等于xᵢ的概率，等于联合概率除以Y的边缘概率。

**例**：

|  | Y=0 | Y=1 |
|--|-----|-----|
| X=0 | 0.2 | 0.1 |
| X=1 | 0.3 | 0.4 |

P(Y=0) = 0.5，P(Y=1) = 0.5

P(X=0 | Y=0) = 0.2 / 0.5 = 0.4

P(X=1 | Y=0) = 0.3 / 0.5 = 0.6

在Y=0的条件下，X有40%概率是0，60%概率是1。

#### 6.4.2 连续型的条件密度

f(x | y) = f(x, y) / fᵧ(y)

读作：在Y等于y的条件下，X的条件密度等于联合密度除以Y的边缘密度。

类似离散情况，只是概率变成密度。

---

### 6.5 随机变量的独立性

两个随机变量X和Y**独立**，当且仅当：

**离散型**：P(X = xᵢ, Y = yⱼ) = P(X = xᵢ) × P(Y = yⱼ)

读作：联合概率等于边缘概率的乘积。

**连续型**：f(x, y) = fₓ(x) × fᵧ(y)

读作：联合密度等于边缘密度的乘积。

**直觉**：知道Y的值，不改变对X的判断。X和Y互不影响。

**等价条件**：X和Y独立 ⟺ F(x, y) = Fₓ(x) × Fᵧ(y)

读作：联合分布函数等于边缘分布函数的乘积。

**例：检验独立性**

|  | Y=0 | Y=1 |
|--|-----|-----|
| X=0 | 0.2 | 0.1 |
| X=1 | 0.3 | 0.4 |

边缘分布：P(X=0)=0.3, P(X=1)=0.7, P(Y=0)=0.5, P(Y=1)=0.5

检验：P(X=0, Y=0) = 0.2，而 P(X=0)×P(Y=0) = 0.3×0.5 = 0.15

0.2 ≠ 0.15，所以X和Y**不独立**。

只要有一个格子不满足"联合=边缘之积"，就不独立。

---

### 6.6 两个随机变量的函数

#### 6.6.1 Z = X + Y 的分布

**离散型**：

P(Z = z) = Σₖ P(X = k, Y = z - k) = Σₖ P(X = k) × P(Y = z - k)（若X、Y独立）

读作：Z等于z的概率，等于所有满足X+Y=z的组合概率之和。

**例：两个骰子点数之和**

P(Z = 7) = P(X=1,Y=6) + P(X=2,Y=5) + ... + P(X=6,Y=1) = 6 × (1/36) = 1/6

**连续型（卷积公式）**：

若X、Y独立，Z = X + Y的密度为：

fᵤ(z) = ∫_{-∞}^{+∞} fₓ(x) × fᵧ(z - x) dx

读作：Z的密度等于X的密度和Y的密度的卷积。

**直觉**：把所有加起来等于z的组合都考虑进去。

#### 6.6.2 正态分布的可加性

若X ~ N(μ₁, σ₁²)，Y ~ N(μ₂, σ₂²)，且X、Y独立，则：

X + Y ~ N(μ₁ + μ₂, σ₁² + σ₂²)

读作：独立正态分布的和仍是正态分布，均值相加，方差相加。

这是正态分布的重要性质，极大简化计算。

**例**：

X ~ N(100, 25)，Y ~ N(50, 16)，X、Y独立

X + Y ~ N(150, 41)

#### 6.6.3 Z = max(X, Y) 和 Z = min(X, Y)

设X、Y独立，分布函数分别为Fₓ(x)和Fᵧ(y)。

**最大值**：

F_max(z) = P(max(X,Y) ≤ z) = P(X ≤ z, Y ≤ z) = Fₓ(z) × Fᵧ(z)

读作：最大值不超过z，等价于X和Y都不超过z。

**最小值**：

F_min(z) = P(min(X,Y) ≤ z) = 1 - P(min(X,Y) > z) = 1 - P(X > z, Y > z)
         = 1 - [1 - Fₓ(z)] × [1 - Fᵧ(y)]

读作：最小值不超过z，等于1减去X和Y都超过z的概率。

**例：两个灯泡的寿命**

设两个灯泡寿命X、Y独立，都服从Exp(λ)。

同时使用，问"至少一个灯泡坏掉"的等待时间Z = min(X, Y)的分布。

F_min(z) = 1 - e^(-λz) × e^(-λz) = 1 - e^(-2λz)

所以 Z ~ Exp(2λ)。

两个指数分布的最小值仍是指数分布，参数变为2λ（坏得更快）。

---

### 6.7 二维正态分布

二维正态分布是最重要的二维连续分布。

**(X, Y) ~ N(μ₁, μ₂, σ₁², σ₂², ρ)**

参数含义：
- μ₁：X的均值
- μ₂：Y的均值
- σ₁²：X的方差
- σ₂²：Y的方差
- ρ：X和Y的相关系数

**性质1**：边缘分布也是正态分布

X ~ N(μ₁, σ₁²)，Y ~ N(μ₂, σ₂²)

**性质2**：X和Y独立 ⟺ ρ = 0

读作：对于二维正态分布，不相关等价于独立！

这是**二维正态分布独有的性质**。一般分布不成立。

**性质3**：线性组合仍是正态

aX + bY + c ~ N(aμ₁ + bμ₂ + c, a²σ₁² + b²σ₂² + 2abρσ₁σ₂)

---

### 本章小结

1. **联合分布**：同时描述多个随机变量的概率规律
2. **边缘分布**：从联合分布"积掉"其他变量得到
3. **条件分布**：已知一个变量时另一个的分布，P(X|Y) = P(X,Y)/P(Y)
4. **独立性**：联合 = 边缘之积
5. **和的分布**：卷积公式
6. **正态可加性**：独立正态之和仍是正态
7. **最大最小值**：F_max = Fₓ × Fᵧ
8. **二维正态**：不相关等价于独立（特有性质）

---

*下一章：大数定律*

---

## 第7章 大数定律

### 7.1 大数定律的直觉

**核心问题**：为什么抛硬币次数足够多时，正面的频率会接近0.5？

这不是显然的。每次抛硬币是独立的，凭什么次数多了就"稳定"？

大数定律回答：当试验次数趋向无穷时，样本均值**依概率收敛**到期望值。

---

### 7.2 切比雪夫不等式（复习）

大数定律的证明依赖切比雪夫不等式。

P(|X - μ| ≥ ε) ≤ σ² / ε²

读作：X偏离均值μ超过ε的概率，不超过方差除以ε的平方。

等价形式：

P(|X - μ| < ε) ≥ 1 - σ² / ε²

这个不等式对**任何分布**都成立。

---

### 7.3 依概率收敛

**定义**：设X₁, X₂, X₃, ... 是一列随机变量，a是常数。

如果对任意ε > 0，有：

lim_{n→∞} P(|Xₙ - a| ≥ ε) = 0

读作：当n趋向无穷时，Xₙ偏离a超过ε的概率趋向于0。

则称Xₙ**依概率收敛**于a，记作 Xₙ →ᴾ a。

**直觉**：n越大，Xₙ落在a附近的可能性越大。

**注意**：这不是说Xₙ一定等于a，而是说"偏离a很远"的概率越来越小。

---

### 7.4 切比雪夫大数定律

**定理**：设X₁, X₂, ..., Xₙ是**两两不相关**的随机变量，且它们的方差有共同上界（Var(Xᵢ) ≤ C）。

设 μᵢ = E(Xᵢ)，则：

(1/n) Σᵢ Xᵢ →ᴾ (1/n) Σᵢ μᵢ

读作：样本均值依概率收敛于期望的均值。

**特别地**，如果X₁, X₂, ..., Xₙ有**相同的期望μ**，则：

X̄ₙ = (X₁ + X₂ + ... + Xₙ) / n →ᴾ μ

读作：样本均值依概率收敛于期望μ。

**证明思路**：

Var(X̄ₙ) = Var((X₁+...+Xₙ)/n) = (1/n²) × nC = C/n → 0

由切比雪夫不等式：

P(|X̄ₙ - μ| ≥ ε) ≤ Var(X̄ₙ) / ε² = C / (nε²) → 0

---

### 7.5 伯努利大数定律

**历史上第一个大数定律**，由雅各布·伯努利于1713年证明。

**定理**：设nₐ是n次独立重复试验中事件A发生的次数，p = P(A)。则：

nₐ / n →ᴾ p

读作：事件A发生的频率依概率收敛于概率p。

**直觉**：频率稳定于概率。这是概率的统计定义的理论基础。

**例：抛硬币**

抛n次硬币，正面出现nₐ次。

当n→∞时，nₐ/n → 0.5（依概率）。

这就是为什么我们说"正面的概率是0.5"。

---

### 7.6 辛钦大数定律

**最常用的形式**，条件最弱。

**定理**：设X₁, X₂, ..., Xₙ是**独立同分布**的随机变量，且期望E(X) = μ存在。则：

X̄ₙ = (X₁ + X₂ + ... + Xₙ) / n →ᴾ μ

读作：独立同分布随机变量的样本均值，依概率收敛于期望。

**与切比雪夫大数定律的区别**：

| 定律 | 条件 |
|------|------|
| 切比雪夫 | 两两不相关，方差有界 |
| 辛钦 | 独立同分布，只需期望存在（不要求方差存在） |

辛钦大数定律条件更弱，但要求"同分布"。

---

### 7.7 大数定律的应用

#### 7.7.1 蒙特卡洛方法

**问题**：计算积分 I = ∫₀¹ f(x) dx

**方法**：
1. 从U(0,1)中随机抽取X₁, X₂, ..., Xₙ
2. 计算 Î = (1/n) Σ f(Xᵢ)
3. 由大数定律，Î →ᴾ E(f(X)) = ∫₀¹ f(x) dx = I

**例**：计算 ∫₀¹ e^x dx

真实值：e - 1 ≈ 1.718

用1000个随机数近似：生成X₁,...,X₁₀₀₀ ~ U(0,1)，计算 (1/1000) Σ e^Xᵢ ≈ 1.72

#### 7.7.2 保险精算

**问题**：保险公司收多少保费才能不亏？

**模型**：
- n个投保人，每人索赔概率p，赔付金额X
- 总赔付 S = X₁ + X₂ + ... + Xₙ

**大数定律告诉我们**：

S/n →ᴾ E(X) × p

当n很大时，平均赔付稳定于 E(X)×p。

所以保费只需略高于 E(X)×p 即可保证不亏（大概率）。

#### 7.7.3 质量控制

**问题**：某产品的废品率是多少？

**方法**：随机抽取n件，检测废品数nₐ。

由伯努利大数定律，nₐ/n ≈ p（废品率）。

n越大，估计越准。

---

### 7.8 强大数定律（选读）

**依概率收敛 vs 几乎必然收敛**

依概率收敛：对每个ε，P(|X̄ₙ - μ| ≥ ε) → 0

几乎必然收敛：P(lim X̄ₙ = μ) = 1

读作：X̄ₙ以概率1收敛到μ。

**强大数定律**：在辛钦大数定律的条件下，有：

P(lim_{n→∞} X̄ₙ = μ) = 1

读作：样本均值几乎必然收敛于期望。

**直觉**：强大数定律比弱大数定律更强，它说的不只是"偏离的概率小"，而是"极限就是μ"。

---

### 本章小结

1. **依概率收敛**：Xₙ →ᴾ a 表示P(|Xₙ-a|≥ε)→0
2. **切比雪夫大数定律**：两两不相关+方差有界 → 均值收敛于期望均值
3. **伯努利大数定律**：频率 →ᴾ 概率
4. **辛钦大数定律**：独立同分布+期望存在 → 样本均值 →ᴾ 期望
5. **应用**：蒙特卡洛积分、保险精算、质量控制
6. **强大数定律**：几乎必然收敛，更强的结论

---

*下一章：中心极限定理*

---

## 第8章 中心极限定理

### 8.1 中心极限定理的直觉

**核心问题**：为什么正态分布无处不在？

回答：因为**独立随机变量之和的分布趋向于正态分布**。

现实中很多量是由大量微小因素累加的结果：
- 人的身高 = 基因因素 + 营养因素 + 环境因素 + ...
- 测量误差 = 仪器误差 + 读数误差 + 环境干扰 + ...
- 股票收益 = 大量交易者决策的累积效应

这些因素独立且微小，根据中心极限定理，总效应近似服从正态分布。

---

### 8.2 独立同分布的中心极限定理（林德伯格-莱维定理）

**定理**：设X₁, X₂, ..., Xₙ是**独立同分布**的随机变量，期望μ，方差σ²（0 < σ² < ∞）。

设 Sₙ = X₁ + X₂ + ... + Xₙ。

则当n→∞时：

(Sₙ - nμ) / (σ√n) →ᵈ N(0, 1)

读作：Sₙ标准化后依分布收敛于标准正态分布。

**等价表述**：

lim_{n→∞} P((Sₙ - nμ) / (σ√n) ≤ x) = Φ(x)

读作：当n趋向无穷时，标准化的Sₙ不超过x的概率趋向于标准正态分布函数Φ(x)。

**用样本均值表述**：

X̄ₙ = Sₙ / n

(X̄ₙ - μ) / (σ/√n) →ᵈ N(0, 1)

读作：样本均值标准化后趋向于标准正态分布。

等价地：

X̄ₙ 近似服从 N(μ, σ²/n)

读作：当n足够大时，样本均值近似正态分布，均值为μ，方差为σ²/n。

---

### 8.3 大数定律 vs 中心极限定理

| 定理 | 说的是什么 |
|------|-----------|
| 大数定律 | X̄ₙ →ᴾ μ（均值收敛到期望） |
| 中心极限定理 | X̄ₙ 的**分布形状**趋向于正态 |

**联系**：

大数定律说"均值稳定"。

中心极限定理说"均值围绕期望的**波动方式**是正态的"。

两者不矛盾：随着n增大，方差σ²/n→0，分布越来越集中在μ附近（大数定律），同时分布形状始终是正态型的（中心极限定理）。

---

### 8.4 德莫弗-拉普拉斯定理

这是中心极限定理的特例，针对**二项分布**。

**定理**：设X ~ B(n, p)，即n次独立试验中成功次数。则当n→∞时：

(X - np) / √(np(1-p)) →ᵈ N(0, 1)

读作：二项分布标准化后趋向于标准正态分布。

**近似公式**：

X 近似服从 N(np, np(1-p))

**例：抛100次硬币**

X = 正面次数，X ~ B(100, 0.5)

E(X) = 50，Var(X) = 25，σ = 5

P(45 ≤ X ≤ 55) = P((45-50)/5 ≤ Z ≤ (55-50)/5)
               = P(-1 ≤ Z ≤ 1)
               = Φ(1) - Φ(-1)
               ≈ 0.8413 - 0.1587
               = 0.6826

约68.3%的概率，正面次数在45到55之间。

**使用条件**：通常要求 np ≥ 5 且 n(1-p) ≥ 5。

---

### 8.5 中心极限定理的应用

#### 8.5.1 正态近似计算

**问题**：计算 P(S₁₀₀ ≤ 350)，其中S₁₀₀ = X₁ + ... + X₁₀₀，Xᵢ独立同分布，E(X)=3，Var(X)=4。

**解法**：

E(S₁₀₀) = 100 × 3 = 300

Var(S₁₀₀) = 100 × 4 = 400，σ = 20

P(S₁₀₀ ≤ 350) = P((S₁₀₀ - 300)/20 ≤ (350-300)/20)
              = P(Z ≤ 2.5)
              = Φ(2.5)
              ≈ 0.9938

#### 8.5.2 样本量估计

**问题**：要使样本均值与总体均值的误差不超过0.5，置信度95%，需要多少样本？

已知：σ = 5

**解法**：

95%置信度对应 z₀.₉₇₅ = 1.96

要求：P(|X̄ - μ| ≤ 0.5) ≥ 0.95

即：P(|Z| ≤ 0.5/(σ/√n)) ≥ 0.95

需要：0.5/(5/√n) ≥ 1.96

解得：√n ≥ 1.96 × 5 / 0.5 = 19.6

所以：n ≥ 385

需要至少385个样本。

#### 8.5.3 选举预测

**问题**：某候选人真实支持率p未知。随机抽样1000人，其中540人支持。能否判断p > 0.5？

**解法**：

设X = 支持人数，X ~ B(1000, p)

样本比例 p̂ = 540/1000 = 0.54

标准误 SE = √(p̂(1-p̂)/n) = √(0.54×0.46/1000) ≈ 0.0158

Z = (p̂ - 0.5) / SE = (0.54 - 0.5) / 0.0158 ≈ 2.53

P(Z > 2.53) ≈ 0.006

在0.05显著性水平下，可以判断 p > 0.5。

---

### 8.6 连续性修正

当用正态分布近似离散分布时，需要做**连续性修正**。

**问题**：X ~ B(100, 0.5)，求P(X = 50)。

**错误做法**：

P(X = 50) = P((50-50)/5 ≤ Z ≤ (50-50)/5) = P(Z = 0) = 0

**正确做法**（连续性修正）：

P(X = 50) ≈ P(49.5 < X < 50.5)
          = P((49.5-50)/5 < Z < (50.5-50)/5)
          = P(-0.1 < Z < 0.1)
          = Φ(0.1) - Φ(-0.1)
          ≈ 0.0796

精确值：C(100,50) × 0.5¹⁰⁰ ≈ 0.0796

**原理**：离散值50对应连续区间[49.5, 50.5]。

---

### 8.7 其他形式的中心极限定理

#### 8.7.1 独立不同分布的情况（林德伯格条件）

即使X₁, X₂, ...不同分布，只要满足一定条件（林德伯格条件），和的标准化仍趋向正态。

这说明正态分布的出现并不依赖于"同分布"，关键是"独立"和"每个因素贡献较小"。

#### 8.7.2 中心极限定理的局限

**不适用的情况**：

1. 方差不存在（如柯西分布）
2. 变量不独立
3. 个别变量贡献过大（如一个变量主导了总和）

**例：柯西分布**

柯西分布的样本均值**不收敛于正态分布**，即使n→∞。

因为柯西分布没有有限的期望和方差。

---

### 本章小结

1. **中心极限定理**：独立同分布之和标准化后趋向正态
2. **公式**：(Sₙ - nμ)/(σ√n) →ᵈ N(0, 1)
3. **样本均值**：X̄ₙ 近似 N(μ, σ²/n)
4. **德莫弗-拉普拉斯**：二项分布的正态近似
5. **连续性修正**：离散→连续时，值k对应[k-0.5, k+0.5]
6. **应用**：正态近似计算、样本量估计、假设检验
7. **局限**：需要方差存在、独立、无主导变量

---

*下一章：抽样与数据*

---

## 第9章 抽样与数据

### 9.1 从概率论到统计学

**概率论与统计学的区别**

| | 概率论 | 统计学 |
|--|--------|--------|
| 已知 | 总体分布 | 样本数据 |
| 求解 | 样本特性 | 总体参数 |
| 方向 | 演绎（从一般到特殊） | 归纳（从特殊到一般） |

**例**：

概率论问题：已知袋中红球占60%，抽10次，问至少7次红球的概率？

统计学问题：抽10次，7次红球，问袋中红球比例是多少？

从本章开始，我们进入统计学领域。

---

### 9.2 总体与样本

**总体（Population）**：研究对象的全体。

**个体**：总体中的每一个成员。

**样本（Sample）**：从总体中抽取的部分个体。

**样本量（Sample Size）**：样本中个体的数量，记作n。

**为什么需要抽样？**

1. 总体太大，无法全部调查（如全国人口）
2. 调查具有破坏性（如灯泡寿命测试）
3. 成本和时间限制

**数学表示**：

设总体的分布为F，或等价地，总体是一个随机变量X。

从总体中抽取n个样本，得到X₁, X₂, ..., Xₙ。

如果抽样是**独立同分布**的，则X₁, X₂, ..., Xₙ是n个独立同分布的随机变量，每个都与总体X同分布。

---

### 9.3 简单随机抽样

**定义**：简单随机抽样满足两个条件：

1. **代表性**：每个个体被抽中的概率相同
2. **独立性**：各次抽取相互独立

**有放回抽样 vs 无放回抽样**

| 方式 | 独立性 | 同分布 |
|------|--------|--------|
| 有放回 | ✓ | ✓ |
| 无放回 | ✗ | ✗（近似） |

实际中，当总体N >> 样本量n时（通常N > 20n），无放回抽样可以近似为有放回抽样。

**常见抽样方法**：

1. **简单随机抽样**：完全随机
2. **分层抽样**：按类别分层，各层内随机
3. **整群抽样**：随机选择若干群体，对选中群体全部调查
4. **系统抽样**：每隔k个抽一个

---

### 9.4 统计量

**定义**：统计量是样本X₁, X₂, ..., Xₙ的函数，且**不含未知参数**。

**关键点**：统计量是随机变量（因为样本是随机的）。

**常用统计量**：

#### 9.4.1 样本均值

X̄ = (X₁ + X₂ + ... + Xₙ) / n = (1/n) Σᵢ Xᵢ

读作：样本均值等于所有样本值之和除以n。

**性质**：

E(X̄) = μ （无偏性）

Var(X̄) = σ² / n

读作：样本均值的期望等于总体期望；样本均值的方差等于总体方差除以n。

#### 9.4.2 样本方差

S² = (1/(n-1)) Σᵢ (Xᵢ - X̄)²

读作：样本方差等于每个样本与样本均值之差的平方和，除以n减1。

**为什么除以n-1而不是n？**

因为用X̄代替了未知的μ，损失了一个自由度。

除以n-1使得E(S²) = σ²（无偏性）。

如果除以n，则 E(S²) = ((n-1)/n)σ² < σ²（有偏，低估）。

#### 9.4.3 样本标准差

S = √S²

读作：样本标准差等于样本方差的平方根。

**注意**：S是σ的**有偏估计**。E(S) ≠ σ（通常略小于σ）。

#### 9.4.4 样本k阶矩

Aₖ = (1/n) Σᵢ Xᵢᵏ

读作：样本k阶原点矩等于所有样本的k次方的平均值。

特别地，A₁ = X̄。

#### 9.4.5 样本k阶中心矩

Bₖ = (1/n) Σᵢ (Xᵢ - X̄)ᵏ

读作：样本k阶中心矩等于所有样本与均值之差的k次方的平均值。

特别地，B₂ = ((n-1)/n)S²。

---

### 9.5 抽样分布

**核心问题**：统计量的分布是什么？

统计量是随机变量，它有自己的分布，称为**抽样分布**。

#### 9.5.1 样本均值的分布

设总体X ~ N(μ, σ²)，X₁, ..., Xₙ是简单随机样本。

则：

X̄ ~ N(μ, σ²/n)

读作：样本均值服从正态分布，均值为μ，方差为σ²/n。

标准化：

(X̄ - μ) / (σ/√n) ~ N(0, 1)

读作：样本均值标准化后服从标准正态分布。

**如果总体不是正态分布？**

由中心极限定理，当n足够大时，X̄近似服从正态分布。

#### 9.5.2 三大抽样分布

为了研究样本方差等统计量的分布，需要引入三个重要分布。

---

### 9.6 卡方分布（χ²分布）

**定义**：设Z₁, Z₂, ..., Zₙ是n个独立的标准正态随机变量。

则：

χ² = Z₁² + Z₂² + ... + Zₙ²

服从**自由度为n的卡方分布**，记作 χ²(n) 或 χₙ²。

读作：n个独立标准正态变量的平方和服从自由度为n的卡方分布。

**性质**：

1. E(χ²(n)) = n
2. Var(χ²(n)) = 2n
3. χ²(n₁) + χ²(n₂) ~ χ²(n₁ + n₂)（可加性，需独立）
4. 图形：右偏，n越大越接近正态

**与样本方差的关系**：

设X₁, ..., Xₙ ~ N(μ, σ²) 独立。则：

(n-1)S² / σ² ~ χ²(n-1)

读作：样本方差乘以(n-1)再除以总体方差，服从自由度n-1的卡方分布。

**为什么是n-1？**

因为Σ(Xᵢ - X̄)² 有一个约束：Σ(Xᵢ - X̄) = 0。

n个变量有1个约束，自由度 = n - 1。

---

### 9.7 t分布（学生t分布）

**定义**：设Z ~ N(0,1)，Y ~ χ²(n)，且Z与Y独立。

则：

T = Z / √(Y/n)

服从**自由度为n的t分布**，记作 t(n)。

读作：标准正态变量除以卡方变量除以自由度的平方根，服从t分布。

**性质**：

1. E(T) = 0（当n > 1）
2. Var(T) = n / (n-2)（当n > 2）
3. 图形：对称，比正态分布更"胖"（尾部更厚）
4. 当n→∞时，t(n) → N(0,1)

**实际应用**：当n ≥ 30时，t分布与正态分布很接近。

**关键公式**：

设X₁, ..., Xₙ ~ N(μ, σ²) 独立。则：

(X̄ - μ) / (S/√n) ~ t(n-1)

读作：样本均值减总体均值，除以样本标准差除以根号n，服从自由度n-1的t分布。

**对比**：

| 公式 | 分布 | 使用条件 |
|------|------|----------|
| (X̄ - μ)/(σ/√n) | N(0,1) | σ已知 |
| (X̄ - μ)/(S/√n) | t(n-1) | σ未知，用S估计 |

**t分布的由来**：

1908年，William Gosset在吉尼斯啤酒厂工作时发现这个分布。

因公司禁止员工发表论文，他用笔名"Student"发表，故称"学生t分布"。

---

### 9.8 F分布

**定义**：设U ~ χ²(n₁)，V ~ χ²(n₂)，且U与V独立。

则：

F = (U/n₁) / (V/n₂)

服从**自由度为(n₁, n₂)的F分布**，记作 F(n₁, n₂)。

读作：两个独立卡方变量分别除以各自自由度后的比值，服从F分布。

**性质**：

1. E(F) = n₂ / (n₂ - 2)（当n₂ > 2）
2. 图形：右偏
3. 若F ~ F(n₁, n₂)，则 1/F ~ F(n₂, n₁)
4. 若T ~ t(n)，则 T² ~ F(1, n)

**应用**：比较两个总体的方差。

设两个独立样本，方差分别为S₁², S₂²，总体方差为σ₁², σ₂²。

则：

(S₁²/σ₁²) / (S₂²/σ₂²) ~ F(n₁-1, n₂-1)

当σ₁² = σ₂²时：

S₁² / S₂² ~ F(n₁-1, n₂-1)

读作：两个样本方差之比服从F分布。

---

### 9.9 正态总体的抽样分布总结

设X₁, ..., Xₙ ~ N(μ, σ²) 独立，样本均值X̄，样本方差S²。

| 统计量 | 分布 |
|--------|------|
| X̄ | N(μ, σ²/n) |
| (X̄ - μ)/(σ/√n) | N(0, 1) |
| (X̄ - μ)/(S/√n) | t(n-1) |
| (n-1)S²/σ² | χ²(n-1) |
| X̄ 与 S² | 相互独立 |

**最后一条很重要**：在正态总体中，样本均值和样本方差是独立的。

这是正态分布的特有性质。

---

### 本章小结

1. **总体与样本**：总体是研究对象全体，样本是抽取的部分
2. **统计量**：样本的函数，不含未知参数，本身是随机变量
3. **样本均值**：E(X̄)=μ，Var(X̄)=σ²/n
4. **样本方差**：除以n-1保证无偏，E(S²)=σ²
5. **卡方分布**：n个标准正态平方和，(n-1)S²/σ² ~ χ²(n-1)
6. **t分布**：σ未知时，(X̄-μ)/(S/√n) ~ t(n-1)
7. **F分布**：比较方差，S₁²/S₂² ~ F(n₁-1, n₂-1)
8. **正态总体**：X̄与S²独立

---

*下一章：参数估计*

---

## 第10章 参数估计

### 10.1 参数估计的基本问题

**问题**：总体分布的形式已知，但参数未知。如何用样本估计参数？

**例**：

已知某产品寿命服从指数分布 Exp(λ)，但λ未知。

抽取n个产品测试寿命，得到X₁, ..., Xₙ。

问：λ是多少？

**两类估计方法**：

1. **点估计**：给出参数的一个具体值
2. **区间估计**：给出参数的一个范围（置信区间）

---

### 10.2 点估计

#### 10.2.1 矩估计法（Method of Moments）

**思想**：用样本矩估计总体矩，然后反解参数。

**步骤**：

1. 写出总体的k阶矩 E(Xᵏ) = g(θ)（θ是待估参数）
2. 用样本矩 Aₖ = (1/n)ΣXᵢᵏ 代替总体矩
3. 解方程 Aₖ = g(θ̂) 得到估计值 θ̂

**例1：正态分布 N(μ, σ²)**

总体一阶矩：E(X) = μ

总体二阶中心矩：Var(X) = σ²

样本矩估计：

μ̂ = X̄

σ̂² = (1/n) Σ(Xᵢ - X̄)² = B₂

**例2：指数分布 Exp(λ)**

E(X) = 1/λ

令 X̄ = 1/λ̂

解得 λ̂ = 1/X̄

读作：λ的矩估计等于样本均值的倒数。

**例3：均匀分布 U(0, θ)**

E(X) = θ/2

令 X̄ = θ̂/2

解得 θ̂ = 2X̄

读作：θ的矩估计等于样本均值的2倍。

**矩估计的优点**：简单，不需要知道分布的具体形式。

**矩估计的缺点**：可能不是最优的估计。

---

#### 10.2.2 最大似然估计法（Maximum Likelihood Estimation, MLE）

**思想**：选择使观测到的样本出现概率最大的参数值。

**似然函数**：

给定样本x₁, ..., xₙ，似然函数定义为：

L(θ) = P(X₁=x₁, ..., Xₙ=xₙ | θ) = ∏ᵢ f(xᵢ; θ)

读作：在参数θ下，观测到样本x₁到xₙ的概率（或密度之积）。

**最大似然估计**：

θ̂_MLE = argmax_θ L(θ)

读作：最大似然估计是使似然函数最大的θ值。

**对数似然函数**：

由于L(θ)是乘积形式，取对数更易处理：

l(θ) = ln L(θ) = Σᵢ ln f(xᵢ; θ)

最大化l(θ)等价于最大化L(θ)。

**求解步骤**：

1. 写出似然函数 L(θ) = ∏ f(xᵢ; θ)
2. 取对数 l(θ) = Σ ln f(xᵢ; θ)
3. 对θ求导，令 dl/dθ = 0
4. 解方程得到 θ̂

**例1：正态分布 N(μ, σ²)**

l(μ, σ²) = -n/2 ln(2π) - n/2 ln(σ²) - (1/2σ²) Σ(xᵢ - μ)²

对μ求导：∂l/∂μ = (1/σ²) Σ(xᵢ - μ) = 0

解得：μ̂ = X̄

对σ²求导：∂l/∂σ² = -n/(2σ²) + (1/2σ⁴) Σ(xᵢ - μ)² = 0

解得：σ̂² = (1/n) Σ(xᵢ - X̄)²

**注意**：MLE的σ̂²是有偏的（除以n而非n-1）。

**例2：指数分布 Exp(λ)**

f(x; λ) = λe^(-λx)

L(λ) = ∏ λe^(-λxᵢ) = λⁿ e^(-λΣxᵢ)

l(λ) = n ln λ - λΣxᵢ

dl/dλ = n/λ - Σxᵢ = 0

解得：λ̂ = n / Σxᵢ = 1/X̄

与矩估计相同。

**例3：均匀分布 U(0, θ)**

f(x; θ) = 1/θ，当 0 ≤ x ≤ θ

L(θ) = (1/θ)ⁿ，当 max(xᵢ) ≤ θ

为使L(θ)最大，θ应尽量小，但必须 θ ≥ max(xᵢ)。

所以：θ̂ = max(X₁, ..., Xₙ) = X₍ₙ₎

读作：θ的MLE是样本的最大值。

**注意**：这与矩估计 θ̂ = 2X̄ 不同！

---

### 10.3 估计量的评价标准

给定一个估计量 θ̂，如何评价它的好坏？

#### 10.3.1 无偏性（Unbiasedness）

**定义**：若 E(θ̂) = θ，称 θ̂ 是 θ 的**无偏估计**。

读作：估计量的期望等于真实参数值。

**直觉**：无偏估计"平均而言"是准确的，不会系统性地高估或低估。

**例**：

- X̄ 是 μ 的无偏估计：E(X̄) = μ ✓
- S² = Σ(Xᵢ-X̄)²/(n-1) 是 σ² 的无偏估计：E(S²) = σ² ✓
- σ̂²_MLE = Σ(Xᵢ-X̄)²/n 是 σ² 的有偏估计：E(σ̂²) = (n-1)/n × σ² ✗

#### 10.3.2 有效性（Efficiency）

**定义**：在所有无偏估计中，方差最小的称为**最有效估计**或**最优无偏估计**。

**比较两个无偏估计**：方差小的更好。

**例**：估计正态总体的μ

- X̄ 的方差：Var(X̄) = σ²/n
- 中位数 的方差：约 πσ²/(2n) ≈ 1.57σ²/n

X̄ 比中位数更有效（方差更小）。

#### 10.3.3 相合性（Consistency）

**定义**：若当n→∞时，θ̂ₙ →ᴾ θ，称 θ̂ 是 θ 的**相合估计**。

读作：随着样本量增大，估计量依概率收敛于真实值。

**直觉**：数据越多，估计越准。

**充分条件**：若 E(θ̂ₙ) → θ 且 Var(θ̂ₙ) → 0，则 θ̂ₙ 相合。

**例**：

- X̄ 是 μ 的相合估计：E(X̄)=μ，Var(X̄)=σ²/n→0 ✓
- S² 是 σ² 的相合估计 ✓

#### 10.3.4 均方误差（MSE）

**定义**：

MSE(θ̂) = E[(θ̂ - θ)²]

读作：估计量与真值之差平方的期望。

**分解公式**：

MSE(θ̂) = Var(θ̂) + [Bias(θ̂)]²

读作：均方误差等于方差加偏差的平方。

其中 Bias(θ̂) = E(θ̂) - θ。

**意义**：MSE综合考虑了偏差和方差。

有时候有偏估计的MSE比无偏估计更小（偏差-方差权衡）。

---

### 10.4 区间估计

#### 10.4.1 置信区间的概念

**问题**：点估计只给一个值，没有"可靠性"信息。

**置信区间**：给出参数可能的范围，附带置信水平。

**定义**：设θ是未知参数，θ̂_L和θ̂_U是两个统计量。

如果 P(θ̂_L < θ < θ̂_U) = 1 - α

称区间(θ̂_L, θ̂_U)是θ的**置信水平为1-α的置信区间**。

读作：有1-α的概率，真实参数θ落在区间(θ̂_L, θ̂_U)内。

**常用置信水平**：90%（α=0.1）、95%（α=0.05）、99%（α=0.01）

**正确理解**：

"95%置信区间"的含义是：如果重复抽样100次，构造100个置信区间，大约有95个区间会包含真实参数。

**不是**说"参数有95%的概率在这个区间内"（参数是固定的，不是随机的）。

#### 10.4.2 单个正态总体均值的置信区间

**情况1：σ²已知**

枢轴量：Z = (X̄ - μ) / (σ/√n) ~ N(0, 1)

由 P(-z_{α/2} < Z < z_{α/2}) = 1 - α

解得 μ 的置信区间：

(X̄ - z_{α/2} × σ/√n, X̄ + z_{α/2} × σ/√n)

读作：均值的置信区间是样本均值加减临界值乘以标准误。

**情况2：σ²未知**

枢轴量：T = (X̄ - μ) / (S/√n) ~ t(n-1)

μ 的置信区间：

(X̄ - t_{α/2}(n-1) × S/√n, X̄ + t_{α/2}(n-1) × S/√n)

读作：用S代替σ，用t分位数代替z分位数。

**例**：某产品抽样16件，平均重量X̄=50g，样本标准差S=4g。求μ的95%置信区间。

t_{0.025}(15) ≈ 2.131

置信区间 = (50 - 2.131×4/√16, 50 + 2.131×4/√16)
         = (50 - 2.131, 50 + 2.131)
         = (47.87, 52.13)

#### 10.4.3 单个正态总体方差的置信区间

枢轴量：χ² = (n-1)S²/σ² ~ χ²(n-1)

由 P(χ²_{1-α/2}(n-1) < χ² < χ²_{α/2}(n-1)) = 1 - α

解得 σ² 的置信区间：

((n-1)S² / χ²_{α/2}(n-1), (n-1)S² / χ²_{1-α/2}(n-1))

读作：方差的置信区间由样本方差乘以(n-1)除以卡方分位数得到。

---

### 10.5 置信区间的宽度

置信区间的宽度取决于：

1. **置信水平**：置信水平越高，区间越宽
2. **样本量**：样本量越大，区间越窄
3. **数据变异性**：方差越大，区间越宽

**公式（σ已知）**：

区间半宽 = z_{α/2} × σ/√n

**样本量估计**：

要使区间半宽不超过d，需要：

n ≥ (z_{α/2} × σ / d)²

**例**：要使95%置信区间半宽不超过1，已知σ=5，需要多少样本？

n ≥ (1.96 × 5 / 1)² = 96.04

至少需要97个样本。

---

### 本章小结

1. **点估计**：给出参数的一个值
2. **矩估计**：令样本矩等于总体矩，反解参数
3. **最大似然估计**：选择使样本出现概率最大的参数
4. **无偏性**：E(θ̂) = θ
5. **有效性**：方差最小
6. **相合性**：n→∞时θ̂→θ
7. **区间估计**：给出参数范围和置信水平
8. **σ已知**：用Z分布，区间为 X̄ ± z_{α/2}×σ/√n
9. **σ未知**：用t分布，区间为 X̄ ± t_{α/2}(n-1)×S/√n

---

*下一章：假设检验*

---

## 第11章 假设检验

### 11.1 假设检验的基本思想

**问题**：有一个关于总体参数的猜想，如何用数据验证？

**例**：

某药厂声称其药片平均重量为5g。质检员随机抽取20片，发现平均重量为4.8g。

问：药厂的声称是否可信？

**基本思路**（反证法）：

1. 先假设药厂说的是真的（μ = 5）
2. 在这个假设下，计算观测到X̄ = 4.8的概率
3. 如果概率很小，说明假设可能是错的

---

### 11.2 基本概念

#### 11.2.1 原假设与备择假设

**原假设（Null Hypothesis）**：记作H₀，是我们要检验的假设。

**备择假设（Alternative Hypothesis）**：记作H₁，是H₀的对立面。

**例**：

- H₀: μ = 5（药片平均重量是5g）
- H₁: μ ≠ 5（药片平均重量不是5g）

**原则**：原假设通常是"无效果"、"无差异"、"等于某值"的假设。

#### 11.2.2 检验统计量

**检验统计量**：用于检验假设的统计量，其分布在H₀成立时已知。

**例**：检验μ = μ₀，σ²已知

Z = (X̄ - μ₀) / (σ/√n)

在H₀成立时，Z ~ N(0, 1)。

#### 11.2.3 拒绝域

**拒绝域（Rejection Region）**：使我们拒绝H₀的检验统计量取值范围。

**临界值**：拒绝域的边界值。

**例**：双侧检验，α = 0.05

拒绝域：|Z| > 1.96

即：Z < -1.96 或 Z > 1.96 时，拒绝H₀。

#### 11.2.4 两类错误

| | H₀为真 | H₀为假 |
|--|--------|--------|
| 拒绝H₀ | 第一类错误（α） | 正确 |
| 接受H₀ | 正确 | 第二类错误（β） |

**第一类错误（Type I Error）**：H₀为真时错误地拒绝H₀。

概率记作α，称为**显著性水平**。

**第二类错误（Type II Error）**：H₀为假时错误地接受H₀。

概率记作β。

**功效（Power）**：1 - β，即H₀为假时正确拒绝的概率。

**关系**：

- α越小，越不容易拒绝H₀，β越大
- 要同时减小α和β，需要增加样本量

**常用α值**：0.05、0.01、0.10

---

### 11.3 假设检验的步骤

1. **建立假设**：确定H₀和H₁
2. **选择检验统计量**：根据问题选择合适的统计量
3. **确定拒绝域**：根据显著性水平α确定临界值
4. **计算统计量**：用样本数据计算检验统计量的值
5. **做出决策**：统计量落入拒绝域则拒绝H₀，否则不拒绝H₀

---

### 11.4 单个正态总体均值的检验

#### 11.4.1 Z检验（σ²已知）

**假设**：H₀: μ = μ₀ vs H₁: μ ≠ μ₀（双侧）

**检验统计量**：

Z = (X̄ - μ₀) / (σ/√n)

在H₀下，Z ~ N(0, 1)。

**拒绝域**（α = 0.05）：|Z| > z_{0.025} = 1.96

**单侧检验**：

- H₁: μ > μ₀，拒绝域：Z > z_α
- H₁: μ < μ₀，拒绝域：Z < -z_α

**例**：某厂生产的零件，长度标准为μ₀ = 10cm，σ = 0.5cm。抽取25件，X̄ = 10.2cm。检验长度是否符合标准（α = 0.05）。

Z = (10.2 - 10) / (0.5/√25) = 0.2 / 0.1 = 2

|Z| = 2 > 1.96

拒绝H₀，认为长度不符合标准。

#### 11.4.2 t检验（σ²未知）

**假设**：H₀: μ = μ₀ vs H₁: μ ≠ μ₀

**检验统计量**：

T = (X̄ - μ₀) / (S/√n)

在H₀下，T ~ t(n-1)。

**拒绝域**（α = 0.05）：|T| > t_{0.025}(n-1)

**例**：随机抽取16名学生，平均成绩X̄ = 72，样本标准差S = 8。检验总体均值是否等于70（α = 0.05）。

T = (72 - 70) / (8/√16) = 2 / 2 = 1

t_{0.025}(15) = 2.131

|T| = 1 < 2.131

不拒绝H₀，没有足够证据说明均值不等于70。

---

### 11.5 单个正态总体方差的检验

**假设**：H₀: σ² = σ₀² vs H₁: σ² ≠ σ₀²

**检验统计量**：

χ² = (n-1)S² / σ₀²

在H₀下，χ² ~ χ²(n-1)。

**拒绝域**：χ² < χ²_{1-α/2}(n-1) 或 χ² > χ²_{α/2}(n-1)

**例**：某产品重量的标准差要求不超过0.5g。抽取10件，样本标准差S = 0.6g。检验是否达标（α = 0.05，单侧）。

H₀: σ² ≤ 0.25 vs H₁: σ² > 0.25

χ² = 9 × 0.36 / 0.25 = 12.96

χ²_{0.05}(9) = 16.919

χ² = 12.96 < 16.919

不拒绝H₀，没有足够证据说明标准差超标。

---

### 11.6 两个正态总体的比较

#### 11.6.1 两个均值之差的检验

**假设**：H₀: μ₁ = μ₂ vs H₁: μ₁ ≠ μ₂

**情况1：σ₁², σ₂²已知**

Z = (X̄₁ - X̄₂) / √(σ₁²/n₁ + σ₂²/n₂)

在H₀下，Z ~ N(0, 1)。

**情况2：σ₁² = σ₂² = σ²未知（方差齐性）**

合并方差：

Sₚ² = [(n₁-1)S₁² + (n₂-1)S₂²] / (n₁ + n₂ - 2)

T = (X̄₁ - X̄₂) / (Sₚ × √(1/n₁ + 1/n₂))

在H₀下，T ~ t(n₁ + n₂ - 2)。

**情况3：σ₁² ≠ σ₂²未知（Welch检验）**

T = (X̄₁ - X̄₂) / √(S₁²/n₁ + S₂²/n₂)

自由度用Welch-Satterthwaite公式近似。

#### 11.6.2 两个方差之比的检验

**假设**：H₀: σ₁² = σ₂² vs H₁: σ₁² ≠ σ₂²

**检验统计量**：

F = S₁² / S₂²

在H₀下，F ~ F(n₁-1, n₂-1)。

**拒绝域**：F < F_{1-α/2}(n₁-1, n₂-1) 或 F > F_{α/2}(n₁-1, n₂-1)

---

### 11.7 p值

**定义**：p值是在H₀成立的条件下，观测到当前结果或更极端结果的概率。

**直觉**：p值越小，数据与H₀越矛盾，越应该拒绝H₀。

**决策规则**：

- p < α：拒绝H₀
- p ≥ α：不拒绝H₀

**例**：Z检验，观测值z = 2.3

双侧p值 = 2 × P(Z > 2.3) = 2 × 0.0107 = 0.0214

若α = 0.05，则p = 0.0214 < 0.05，拒绝H₀。

**p值的优点**：

1. 不依赖于预先选定的α
2. 提供了更多信息（证据强度）
3. 便于报告和比较

**p值的误解**：

- p值**不是**H₀为真的概率
- p值**不是**结果由偶然产生的概率
- p值小**不等于**效应大

---

### 11.8 检验与置信区间的关系

**定理**：对于双侧检验 H₀: μ = μ₀ vs H₁: μ ≠ μ₀

在显著性水平α下拒绝H₀ ⟺ μ₀不在1-α置信区间内

**直觉**：

- 置信区间包含"合理"的参数值
- 如果H₀假设的值不在置信区间内，说明它"不合理"，应该拒绝

**例**：95%置信区间为(10.1, 10.5)

- 检验H₀: μ = 10，因为10不在区间内，拒绝H₀
- 检验H₀: μ = 10.3，因为10.3在区间内，不拒绝H₀

---

### 11.9 常见检验方法速查

| 检验问题 | 条件 | 检验统计量 | 分布 |
|----------|------|------------|------|
| μ = μ₀ | σ已知 | Z = (X̄-μ₀)/(σ/√n) | N(0,1) |
| μ = μ₀ | σ未知 | T = (X̄-μ₀)/(S/√n) | t(n-1) |
| σ² = σ₀² | | χ² = (n-1)S²/σ₀² | χ²(n-1) |
| μ₁ = μ₂ | σ₁=σ₂未知 | T = (X̄₁-X̄₂)/(Sₚ√(1/n₁+1/n₂)) | t(n₁+n₂-2) |
| σ₁² = σ₂² | | F = S₁²/S₂² | F(n₁-1,n₂-1) |

---

### 本章小结

1. **假设检验**：用数据检验关于参数的猜想
2. **原假设H₀**：要检验的假设，通常是"无效果"假设
3. **两类错误**：α=P(拒绝H₀|H₀真)，β=P(接受H₀|H₀假)
4. **Z检验**：σ已知时检验均值
5. **t检验**：σ未知时检验均值
6. **χ²检验**：检验方差
7. **F检验**：比较两个方差
8. **p值**：观测到当前或更极端结果的概率
9. **检验与置信区间**：拒绝H₀ ⟺ 假设值不在置信区间内

---

*下一章：方差分析*

---

## 第12章 方差分析

### 12.1 方差分析的问题

**问题**：比较多个总体的均值是否相等。

**例**：

三种教学方法A、B、C，各选20名学生测试。问三种方法的效果是否有差异？

**为什么不用多次t检验？**

如果有k个组，需要做C(k,2) = k(k-1)/2次两两比较。

每次检验α = 0.05，多次检验会累积第一类错误：

总体犯错率 ≈ 1 - (1-α)^{k(k-1)/2}

当k = 5时，需要10次比较，总错误率约40%！

方差分析（ANOVA）一次检验所有组，控制总体错误率。

---

### 12.2 单因素方差分析

#### 12.2.1 模型设定

有k个处理（组），第i组有nᵢ个观测值。

**模型**：

Xᵢⱼ = μᵢ + εᵢⱼ

读作：第i组第j个观测值等于该组均值加上随机误差。

其中 εᵢⱼ ~ N(0, σ²) 独立同分布。

**假设**：

H₀: μ₁ = μ₂ = ... = μₖ（所有组均值相等）

H₁: 至少有两个均值不相等

#### 12.2.2 基本思想

**关键洞察**：总变异 = 组间变异 + 组内变异

如果H₀为真（各组均值相等），则组间变异应该较小。

如果H₀为假（各组均值不等），则组间变异应该较大。

**总变异的分解**：

SST = SSA + SSE

其中：

- SST（Total）= Σᵢⱼ(Xᵢⱼ - X̄)²，总平方和
- SSA（Among）= Σᵢ nᵢ(X̄ᵢ - X̄)²，组间平方和
- SSE（Error）= Σᵢⱼ(Xᵢⱼ - X̄ᵢ)²，组内平方和

读作：总平方和等于组间平方和加组内平方和。

#### 12.2.3 均方与F统计量

**均方**：平方和除以自由度

MSA = SSA / (k-1)，组间均方

MSE = SSE / (n-k)，组内均方

其中 n = Σnᵢ 是总样本量。

**F统计量**：

F = MSA / MSE

在H₀下，F ~ F(k-1, n-k)。

读作：组间均方与组内均方之比服从F分布。

**拒绝域**：F > F_α(k-1, n-k)

**直觉**：

- F大说明组间变异远大于组内变异，各组均值可能不同
- F接近1说明组间变异与组内变异差不多，均值可能相等

#### 12.2.4 方差分析表

| 来源 | 平方和 | 自由度 | 均方 | F值 |
|------|--------|--------|------|-----|
| 组间 | SSA | k-1 | MSA | F=MSA/MSE |
| 组内 | SSE | n-k | MSE | |
| 总计 | SST | n-1 | | |

**例**：三组数据

| 组A | 组B | 组C |
|-----|-----|-----|
| 5 | 8 | 6 |
| 6 | 7 | 5 |
| 4 | 9 | 7 |

各组均值：X̄_A = 5, X̄_B = 8, X̄_C = 6

总均值：X̄ = (15+24+18)/9 = 19/3 ≈ 6.33

SSA = 3×(5-6.33)² + 3×(8-6.33)² + 3×(6-6.33)²
    = 3×1.78 + 3×2.79 + 3×0.11
    = 14.0

SSE = (5-5)² + (6-5)² + (4-5)² + (8-8)² + (7-8)² + (9-8)² + (6-6)² + (5-6)² + (7-6)²
    = 0 + 1 + 1 + 0 + 1 + 1 + 0 + 1 + 1
    = 6

MSA = 14/2 = 7

MSE = 6/6 = 1

F = 7/1 = 7

F_{0.05}(2, 6) ≈ 5.14

F = 7 > 5.14，拒绝H₀，认为三组均值有显著差异。

---

### 12.3 方差分析的假设条件

1. **正态性**：各组数据来自正态分布
2. **方差齐性**：各组方差相等（σ₁² = σ₂² = ... = σₖ² = σ²）
3. **独立性**：各观测值相互独立

**检验方法**：

- 正态性：Shapiro-Wilk检验、Q-Q图
- 方差齐性：Levene检验、Bartlett检验

**稳健性**：

方差分析对正态性的偏离有一定的稳健性，尤其是样本量较大时（中心极限定理）。

但对方差齐性较敏感。

---

### 12.4 效应量

**问题**：p值只告诉我们差异是否"显著"，不告诉我们差异"有多大"。

**η²（eta squared）**：

η² = SSA / SST

读作：组间变异占总变异的比例。

**解释**：

- η² ≈ 0.01：小效应
- η² ≈ 0.06：中等效应
- η² ≈ 0.14：大效应

**例**：上例中 η² = 14 / 20 = 0.7，是非常大的效应。

---

### 12.5 多重比较

**问题**：方差分析拒绝H₀后，只知道"至少有两组不同"，但哪两组不同？

**多重比较方法**：

#### 12.5.1 LSD检验（最小显著差异）

两组均值之差的临界值：

LSD = t_{α/2}(n-k) × √(MSE × (1/nᵢ + 1/nⱼ))

如果 |X̄ᵢ - X̄ⱼ| > LSD，则认为第i组和第j组有显著差异。

**问题**：没有控制总体错误率。

#### 12.5.2 Bonferroni校正

将显著性水平除以比较次数：

α' = α / m

其中 m = k(k-1)/2 是两两比较的次数。

**保守**，检验功效较低。

#### 12.5.3 Tukey HSD

专门为方差分析设计的多重比较方法，控制家族错误率。

临界值：

q × √(MSE / n)

其中q是学生化范围分布的分位数。

---

### 12.6 双因素方差分析（简介）

**问题**：同时考虑两个因素对响应变量的影响。

**例**：研究教学方法（3种）和学生性别（2种）对成绩的影响。

**模型**：

Xᵢⱼₖ = μ + αᵢ + βⱼ + (αβ)ᵢⱼ + εᵢⱼₖ

其中：

- αᵢ：因素A的主效应
- βⱼ：因素B的主效应
- (αβ)ᵢⱼ：交互效应

**假设检验**：

- H₀: 所有αᵢ = 0（因素A无主效应）
- H₀: 所有βⱼ = 0（因素B无主效应）
- H₀: 所有(αβ)ᵢⱼ = 0（无交互效应）

**交互效应的直觉**：

如果教学方法对男生和女生的效果不同，就存在交互效应。

例如：方法A对男生效果好，方法B对女生效果好。

---

### 12.7 方差分析与回归分析的联系

方差分析和回归分析本质上是同一框架：**一般线性模型**。

**方差分析**：自变量是分类变量（组别）

**回归分析**：自变量是连续变量

**虚拟变量编码**：

将k个组编码为k-1个虚拟变量：

D₁ = 1 若属于组1，否则 = 0

D₂ = 1 若属于组2，否则 = 0

...

则方差分析可以写成回归形式：

Y = β₀ + β₁D₁ + β₂D₂ + ... + ε

检验所有βᵢ = 0等价于方差分析的F检验。

---

### 本章小结

1. **方差分析**：比较多个组均值是否相等
2. **基本思想**：总变异 = 组间变异 + 组内变异
3. **F统计量**：F = MSA/MSE，组间均方与组内均方之比
4. **拒绝H₀**：F > F_α(k-1, n-k)
5. **假设条件**：正态性、方差齐性、独立性
6. **效应量**：η² = SSA/SST
7. **多重比较**：LSD、Bonferroni、Tukey HSD
8. **双因素方差分析**：考虑主效应和交互效应

---

*下一章：回归分析*

---

## 第13章 回归分析

### 13.1 回归分析的问题

**问题**：研究变量之间的关系，用一个或多个自变量预测因变量。

**例**：

- 用广告投入预测销售额
- 用身高预测体重
- 用温度、湿度预测用电量

**回归 vs 相关**：

| | 相关分析 | 回归分析 |
|--|----------|----------|
| 目的 | 衡量关联强度 | 建立预测模型 |
| 变量角色 | 对称 | 区分自变量和因变量 |
| 输出 | 相关系数 | 回归方程 |

---

### 13.2 一元线性回归

#### 13.2.1 模型

**模型**：

Y = β₀ + β₁X + ε

读作：Y等于截距β₀加上斜率β₁乘以X，再加上随机误差ε。

其中：

- Y：因变量（响应变量）
- X：自变量（解释变量）
- β₀：截距
- β₁：斜率
- ε ~ N(0, σ²)：随机误差

**假设**：

1. **线性**：Y与X的关系是线性的
2. **独立**：各观测值相互独立
3. **正态**：误差服从正态分布
4. **等方差**：误差的方差恒定（同方差性）

缩写：LINE（Linearity, Independence, Normality, Equal variance）

#### 13.2.2 最小二乘估计

**目标**：找到β̂₀和β̂₁，使残差平方和最小。

RSS = Σ(Yᵢ - Ŷᵢ)² = Σ(Yᵢ - β̂₀ - β̂₁Xᵢ)²

读作：残差平方和等于实际值与预测值之差的平方和。

**求解**：对β₀和β₁求偏导，令其为0。

**结果**：

β̂₁ = Σ(Xᵢ - X̄)(Yᵢ - Ȳ) / Σ(Xᵢ - X̄)² = Sxy / Sxx

β̂₀ = Ȳ - β̂₁X̄

读作：斜率等于X和Y的协方差除以X的方差；截距等于Y的均值减去斜率乘以X的均值。

**回归方程**：

Ŷ = β̂₀ + β̂₁X

读作：Y的预测值等于截距加斜率乘以X。

**例**：5个数据点 (1,2), (2,3), (3,5), (4,4), (5,6)

X̄ = 3, Ȳ = 4

Sxy = (1-3)(2-4) + (2-3)(3-4) + (3-3)(5-4) + (4-3)(4-4) + (5-3)(6-4)
    = 4 + 1 + 0 + 0 + 4 = 9

Sxx = (1-3)² + (2-3)² + (3-3)² + (4-3)² + (5-3)² = 4+1+0+1+4 = 10

β̂₁ = 9/10 = 0.9

β̂₀ = 4 - 0.9×3 = 1.3

回归方程：Ŷ = 1.3 + 0.9X

#### 13.2.3 回归系数的解释

**β̂₁的解释**：X每增加1个单位，Y平均增加β̂₁个单位。

**β̂₀的解释**：当X=0时，Y的预测值。（不一定有实际意义）

**例**：Ŷ = 1.3 + 0.9X

X每增加1，Y平均增加0.9。

---

### 13.3 回归分析的推断

#### 13.3.1 误差方差的估计

**残差**：eᵢ = Yᵢ - Ŷᵢ

**均方误差**：

MSE = RSS / (n-2) = Σeᵢ² / (n-2)

读作：残差平方和除以n-2。

MSE是σ²的无偏估计。

**为什么是n-2？**

因为估计了2个参数（β₀和β₁），损失了2个自由度。

#### 13.3.2 斜率的假设检验

**假设**：H₀: β₁ = 0 vs H₁: β₁ ≠ 0

若β₁ = 0，说明X与Y无线性关系。

**检验统计量**：

T = β̂₁ / SE(β̂₁)

其中 SE(β̂₁) = √(MSE / Sxx)

在H₀下，T ~ t(n-2)。

**拒绝域**：|T| > t_{α/2}(n-2)

**例**：上例中 n=5，MSE需要先计算残差。

Ŷ值：2.2, 3.1, 4.0, 4.9, 5.8

残差：-0.2, -0.1, 1.0, -0.9, 0.2

RSS = 0.04 + 0.01 + 1.0 + 0.81 + 0.04 = 1.9

MSE = 1.9 / 3 = 0.633

SE(β̂₁) = √(0.633/10) = 0.252

T = 0.9 / 0.252 = 3.57

t_{0.025}(3) = 3.182

|T| = 3.57 > 3.182，拒绝H₀，斜率显著不为0。

#### 13.3.3 斜率的置信区间

β₁的95%置信区间：

β̂₁ ± t_{α/2}(n-2) × SE(β̂₁)

**例**：0.9 ± 3.182 × 0.252 = (0.10, 1.70)

---

### 13.4 回归模型的评价

#### 13.4.1 决定系数 R²

**定义**：

R² = 1 - RSS/SST = SSR/SST

其中：

- SST = Σ(Yᵢ - Ȳ)²，总平方和
- SSR = Σ(Ŷᵢ - Ȳ)²，回归平方和
- RSS = Σ(Yᵢ - Ŷᵢ)²，残差平方和

且 SST = SSR + RSS。

**解释**：R²表示Y的变异中被X解释的比例。

**取值范围**：0 ≤ R² ≤ 1

- R² = 0：X完全不能解释Y
- R² = 1：X完全解释Y（所有点在直线上）

**与相关系数的关系**：

对于一元线性回归，R² = r²（相关系数的平方）。

**例**：

SST = (2-4)² + (3-4)² + (5-4)² + (4-4)² + (6-4)² = 4+1+1+0+4 = 10

RSS = 1.9（已计算）

R² = 1 - 1.9/10 = 0.81

81%的Y的变异可以被X解释。

#### 13.4.2 调整R²

**问题**：增加自变量总会使R²增加，即使新变量无用。

**调整R²**：

R²_adj = 1 - (RSS/(n-p-1)) / (SST/(n-1))
       = 1 - (n-1)/(n-p-1) × (1-R²)

其中p是自变量个数。

调整R²会惩罚不必要的自变量。

---

### 13.5 多元线性回归

#### 13.5.1 模型

**模型**：

Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ + ε

读作：Y等于截距加各个自变量乘以相应系数之和，再加误差。

**矩阵形式**：

Y = Xβ + ε

其中：

- Y是n×1向量
- X是n×(p+1)设计矩阵
- β是(p+1)×1参数向量
- ε是n×1误差向量

**最小二乘估计**：

β̂ = (X'X)⁻¹X'Y

读作：β的估计等于X转置乘X的逆，再乘X转置乘Y。

#### 13.5.2 系数解释

**偏回归系数βⱼ的解释**：

在其他变量保持不变的条件下，Xⱼ增加1个单位，Y平均变化βⱼ个单位。

**关键**："其他变量保持不变"。

这与一元回归的解释不同！

**例**：Y = 5 + 2X₁ - 3X₂

- 固定X₂不变，X₁增加1，Y增加2
- 固定X₁不变，X₂增加1，Y减少3

#### 13.5.3 多重共线性

**定义**：自变量之间高度相关。

**后果**：

1. 系数估计不稳定（方差大）
2. 系数符号可能反常
3. 单个系数不显著，但整体模型显著

**诊断**：

- 方差膨胀因子（VIF）：VIF > 10表示严重共线性
- 条件数

**处理**：

1. 删除高度相关的变量
2. 主成分回归
3. 岭回归（Ridge Regression）

---

### 13.6 残差分析

#### 13.6.1 残差图

**残差 vs 拟合值图**：

- 理想：随机散布在0附近
- 漏斗形：异方差
- 曲线形：非线性

**残差 vs 自变量图**：

- 检查是否遗漏了非线性项

**Q-Q图**：

- 检查残差的正态性
- 点应近似落在直线上

#### 13.6.2 影响点

**离群值（Outlier）**：Y值异常

**高杠杆点（High Leverage）**：X值异常

**影响点（Influential Point）**：同时是离群值和高杠杆点

**Cook距离**：衡量删除某点后对回归的影响

Cook's D > 1 或 > 4/n 表示影响大。

---

### 13.7 模型选择

**问题**：应该包含哪些自变量？

**原则**：

- 太少变量：欠拟合，偏差大
- 太多变量：过拟合，方差大

**方法**：

#### 13.7.1 逐步回归

**前向选择**：从空模型开始，逐步加入最显著的变量

**后向消除**：从全模型开始，逐步删除最不显著的变量

**双向选择**：结合前向和后向

#### 13.7.2 信息准则

**AIC（Akaike Information Criterion）**：

AIC = n ln(RSS/n) + 2(p+1)

**BIC（Bayesian Information Criterion）**：

BIC = n ln(RSS/n) + (p+1) ln(n)

选择AIC或BIC最小的模型。

BIC对参数数量的惩罚更大，倾向于选择更简单的模型。

---

### 本章小结

1. **一元线性回归**：Y = β₀ + β₁X + ε
2. **最小二乘**：使残差平方和最小
3. **斜率公式**：β̂₁ = Sxy/Sxx
4. **斜率检验**：T = β̂₁/SE(β̂₁) ~ t(n-2)
5. **R²**：Y的变异被X解释的比例
6. **多元回归**：Y = β₀ + β₁X₁ + ... + βₚXₚ + ε
7. **系数解释**："其他变量不变时"的边际效应
8. **残差分析**：检验模型假设
9. **模型选择**：AIC、BIC、逐步回归

---

*下一章：相关性分析*

---

## 第14章 相关性分析

### 14.1 相关性的概念

**相关性**：两个变量之间的关联程度。

**注意**：相关不等于因果！

**例**：

- 冰淇淋销量与溺水人数正相关 → 原因是夏天（混杂变量）
- 鸡叫与日出正相关 → 鸡叫不是日出的原因

---

### 14.2 Pearson相关系数

#### 14.2.1 定义

**总体相关系数**：

ρ = Cov(X, Y) / (σₓ × σᵧ) = E[(X-μₓ)(Y-μᵧ)] / (σₓσᵧ)

读作：X和Y的协方差除以各自标准差的乘积。

**样本相关系数**：

r = Σ(Xᵢ - X̄)(Yᵢ - Ȳ) / √[Σ(Xᵢ - X̄)² × Σ(Yᵢ - Ȳ)²]
  = Sxy / √(Sxx × Syy)

读作：X和Y的样本协方差除以各自样本标准差的乘积。

#### 14.2.2 性质

1. **取值范围**：-1 ≤ r ≤ 1
2. **对称性**：r(X, Y) = r(Y, X)
3. **无量纲**：r与X、Y的单位无关
4. **线性变换不变性**：r(aX+b, cY+d) = ±r(X, Y)（符号取决于a和c的符号）

#### 14.2.3 解释

| r的值 | 解释 |
|-------|------|
| r = 1 | 完全正线性相关 |
| r = -1 | 完全负线性相关 |
| r = 0 | 无线性相关 |
| 0.8 ≤ |r| < 1 | 强相关 |
| 0.5 ≤ |r| < 0.8 | 中等相关 |
| 0.3 ≤ |r| < 0.5 | 弱相关 |
| |r| < 0.3 | 极弱相关 |

**警告**：r只衡量**线性**相关。r = 0不代表X和Y无关！

**例**：Y = X²，X在[-1, 1]均匀分布

X和Y有完美的函数关系，但r = 0（因为关系是非线性的）。

#### 14.2.4 相关性的假设检验

**假设**：H₀: ρ = 0 vs H₁: ρ ≠ 0

**检验统计量**：

T = r × √(n-2) / √(1-r²)

在H₀下，T ~ t(n-2)。

**拒绝域**：|T| > t_{α/2}(n-2)

**例**：n = 20，r = 0.5

T = 0.5 × √18 / √(1-0.25) = 0.5 × 4.24 / 0.866 = 2.45

t_{0.025}(18) = 2.101

|T| = 2.45 > 2.101，拒绝H₀，相关性显著。

---

### 14.3 Spearman秩相关系数

#### 14.3.1 定义

将数据按大小排序，用**秩（rank）**代替原始值，再计算Pearson相关系数。

**秩**：数据从小到大排列的位置。

**公式**（无相同秩时）：

rₛ = 1 - 6Σdᵢ² / [n(n²-1)]

其中 dᵢ = rank(Xᵢ) - rank(Yᵢ)，第i对数据的秩之差。

读作：1减去6倍秩差平方和除以n(n²-1)。

#### 14.3.2 优点

1. **不要求正态分布**
2. **对异常值稳健**
3. **可以衡量单调关系**（不仅是线性）

**例**：

| X | Y | rank(X) | rank(Y) | d | d² |
|---|---|---------|---------|---|----|
| 10 | 25 | 1 | 2 | -1 | 1 |
| 20 | 20 | 2 | 1 | 1 | 1 |
| 30 | 35 | 3 | 3 | 0 | 0 |
| 40 | 45 | 4 | 4 | 0 | 0 |
| 50 | 60 | 5 | 5 | 0 | 0 |

Σd² = 2

rₛ = 1 - 6×2 / [5×24] = 1 - 12/120 = 0.9

强正相关。

#### 14.3.3 Spearman vs Pearson

| | Pearson r | Spearman rₛ |
|--|-----------|-------------|
| 衡量的关系 | 线性 | 单调 |
| 分布假设 | 正态 | 无 |
| 对异常值 | 敏感 | 稳健 |
| 使用场景 | 连续变量 | 有序变量、非正态 |

---

### 14.4 Kendall τ相关系数

#### 14.4.1 定义

基于**一致对**和**不一致对**的概念。

对于两对数据(Xᵢ, Yᵢ)和(Xⱼ, Yⱼ)：

- **一致**：(Xᵢ - Xⱼ)(Yᵢ - Yⱼ) > 0（同向变化）
- **不一致**：(Xᵢ - Xⱼ)(Yᵢ - Yⱼ) < 0（反向变化）

**Kendall τ**：

τ = (C - D) / [n(n-1)/2]

其中C是一致对数，D是不一致对数。

读作：一致对数减不一致对数，除以总配对数。

#### 14.4.2 特点

1. 取值范围：-1 ≤ τ ≤ 1
2. 比Spearman更保守（绝对值通常更小）
3. 在小样本下有更好的统计性质

---

### 14.5 偏相关系数

**问题**：控制第三个变量后，两个变量的相关性。

**例**：

身高X与体重Y的相关系数r = 0.7。

但年龄Z影响身高和体重。

控制年龄后的偏相关是多少？

**公式**：

r_{XY·Z} = (r_{XY} - r_{XZ}×r_{YZ}) / √[(1-r_{XZ}²)(1-r_{YZ}²)]

读作：X和Y控制Z后的偏相关，等于原相关减去各自与Z相关的乘积，除以修正因子。

**解释**：偏相关反映"纯粹的"X与Y的关系，排除了Z的影响。

---

### 14.6 相关矩阵

**多变量情况**：有p个变量X₁, X₂, ..., Xₚ。

**相关矩阵**：

R = [rᵢⱼ]

其中rᵢⱼ = r(Xᵢ, Xⱼ)。

**性质**：

1. 对角线元素 rᵢᵢ = 1
2. 对称矩阵 rᵢⱼ = rⱼᵢ
3. 正定或半正定

**例**：三个变量的相关矩阵

```
      X₁    X₂    X₃
X₁  [1.00  0.80  0.30]
X₂  [0.80  1.00  0.50]
X₃  [0.30  0.50  1.00]
```

X₁和X₂高度相关（0.80），X₁和X₃弱相关（0.30）。

---

### 14.7 相关性的可视化

#### 14.7.1 散点图

最直观的展示方式。

- 点越集中在一条线附近，相关性越强
- 斜率为正→正相关，斜率为负→负相关

#### 14.7.2 相关热图

用颜色表示相关系数大小。

- 深红色：强正相关
- 深蓝色：强负相关
- 白色：无相关

#### 14.7.3 散点图矩阵

多变量时，展示所有两两变量的散点图。

---

### 14.8 相关分析的常见陷阱

#### 14.8.1 相关≠因果

**例**：冰淇淋销量与溺水人数正相关。

原因：两者都受夏天气温影响（混杂变量）。

不能说"吃冰淇淋导致溺水"。

#### 14.8.2 辛普森悖论

**现象**：分组数据的相关性与合并后相反。

**例**：

- A组：X↑，Y↑（正相关）
- B组：X↑，Y↑（正相关）
- 合并后：X↑，Y↓（负相关）

原因：组间差异导致混淆。

#### 14.8.3 限制范围

在部分数据范围内计算的相关系数可能失真。

**例**：身高与收入在全人口中弱正相关。

但如果只看NBA球员（身高范围很窄），可能看不到相关性。

#### 14.8.4 异常值影响

一个异常值可能极大地影响Pearson r。

**建议**：

- 画散点图检查
- 考虑用Spearman相关

---

### 本章小结

1. **Pearson r**：衡量线性相关，-1到1
2. **检验**：T = r√(n-2)/√(1-r²) ~ t(n-2)
3. **Spearman rₛ**：基于秩，衡量单调关系，对异常值稳健
4. **Kendall τ**：基于一致/不一致对
5. **偏相关**：控制第三变量后的相关
6. **相关矩阵**：多变量两两相关系数
7. **陷阱**：相关≠因果、辛普森悖论、范围限制、异常值

---

*下一章：非参数检验*

---

## 第15章 非参数检验

### 15.1 什么是非参数检验

**参数检验 vs 非参数检验**

| | 参数检验 | 非参数检验 |
|--|----------|------------|
| 假设 | 总体服从特定分布（如正态） | 不假设分布形式 |
| 检验对象 | 参数（μ, σ²） | 分布、中位数、秩 |
| 数据要求 | 连续、正态 | 可以是有序数据 |
| 效率 | 假设成立时更高效 | 更稳健 |

**何时使用非参数检验？**

1. 数据明显非正态
2. 样本量很小，无法验证正态性
3. 数据是有序的（如评分1-5）
4. 存在异常值

---

### 15.2 符号检验

**问题**：检验中位数是否等于某个值。

**假设**：H₀: M = M₀ vs H₁: M ≠ M₀

**方法**：

1. 计算每个观测值与M₀的差值
2. 记录正差和负差的个数
3. 如果H₀为真，正负差个数应该接近

**检验统计量**：

设n₊是正差个数，n₋是负差个数（忽略等于M₀的）。

在H₀下，n₊ ~ B(n, 0.5)。

**例**：10个数据，检验中位数是否为50。

数据：45, 52, 48, 55, 60, 42, 58, 47, 53, 49

与50比较：-, +, -, +, +, -, +, -, +, -

n₊ = 5，n₋ = 5

P(|n₊ - 5| ≥ 0 | H₀) = 1

不拒绝H₀，中位数可能是50。

---

### 15.3 Wilcoxon符号秩检验

**改进**：不仅考虑符号，还考虑差值大小。

**步骤**：

1. 计算 dᵢ = Xᵢ - M₀
2. 按|dᵢ|从小到大排秩
3. 给秩加上原来的符号
4. 计算正秩和W₊和负秩和W₋

**检验统计量**：W = min(W₊, W₋)

**例**：

| X | d = X-50 | |d| | 秩 | 带符号秩 |
|---|----------|-----|-----|----------|
| 45 | -5 | 5 | 4 | -4 |
| 52 | 2 | 2 | 1.5 | +1.5 |
| 48 | -2 | 2 | 1.5 | -1.5 |
| 55 | 5 | 5 | 4 | +4 |
| 60 | 10 | 10 | 6 | +6 |
| 42 | -8 | 8 | 5 | -5 |

W₊ = 1.5 + 4 + 6 = 11.5

W₋ = 4 + 1.5 + 5 = 10.5

W = min(11.5, 10.5) = 10.5

查表或用正态近似判断是否显著。

---

### 15.4 Wilcoxon秩和检验（Mann-Whitney U检验）

**问题**：比较两个独立样本的分布是否相同。

**假设**：H₀: 两组分布相同 vs H₁: 两组分布不同

**步骤**：

1. 将两组数据合并排秩
2. 计算各组的秩和
3. 检验秩和是否显著偏离期望

**检验统计量**：

U₁ = n₁n₂ + n₁(n₁+1)/2 - R₁

U₂ = n₁n₂ + n₂(n₂+1)/2 - R₂

其中R₁, R₂是两组的秩和。

U = min(U₁, U₂)

**大样本近似**：

E(U) = n₁n₂/2

Var(U) = n₁n₂(n₁+n₂+1)/12

Z = (U - E(U)) / √Var(U) 近似 N(0,1)

**例**：

组A：3, 5, 7（n₁=3）

组B：8, 10, 12, 15（n₂=4）

合并排秩：3(1), 5(2), 7(3), 8(4), 10(5), 12(6), 15(7)

R₁ = 1+2+3 = 6

R₂ = 4+5+6+7 = 22

U₁ = 3×4 + 3×4/2 - 6 = 12 + 6 - 6 = 12

U₂ = 3×4 + 4×5/2 - 22 = 12 + 10 - 22 = 0

U = min(12, 0) = 0

U很小，说明组B的秩普遍较高，两组可能有差异。

---

### 15.5 Kruskal-Wallis检验

**问题**：比较多个（≥3）独立样本的分布。

相当于方差分析的非参数版本。

**假设**：H₀: 各组分布相同 vs H₁: 至少两组不同

**步骤**：

1. 将所有数据合并排秩
2. 计算各组秩和Rᵢ
3. 计算H统计量

**检验统计量**：

H = [12 / (N(N+1))] × Σ(Rᵢ²/nᵢ) - 3(N+1)

其中N是总样本量。

在H₀下，H近似服从χ²(k-1)，k是组数。

**拒绝域**：H > χ²_α(k-1)

---

### 15.6 卡方检验

#### 15.6.1 拟合优度检验

**问题**：数据是否服从某个理论分布？

**假设**：H₀: 数据服从指定分布

**方法**：

1. 将数据分成k类
2. 计算各类的观测频数Oᵢ和期望频数Eᵢ
3. 计算卡方统计量

**检验统计量**：

χ² = Σ(Oᵢ - Eᵢ)² / Eᵢ

在H₀下，χ² ~ χ²(k-1-m)，m是估计的参数个数。

**例：检验骰子是否均匀**

投60次骰子，各面朝上次数：8, 12, 10, 11, 9, 10

期望频数：E = 60/6 = 10

χ² = (8-10)²/10 + (12-10)²/10 + ... + (10-10)²/10
   = 0.4 + 0.4 + 0 + 0.1 + 0.1 + 0
   = 1.0

χ²_{0.05}(5) = 11.07

χ² = 1.0 < 11.07，不拒绝H₀，骰子可能是均匀的。

#### 15.6.2 独立性检验

**问题**：两个分类变量是否独立？

**假设**：H₀: 两变量独立

**数据形式**：列联表

**例**：性别与是否吸烟

|  | 吸烟 | 不吸烟 | 合计 |
|--|------|--------|------|
| 男 | 40 | 60 | 100 |
| 女 | 20 | 80 | 100 |
| 合计 | 60 | 140 | 200 |

**期望频数**（若独立）：

E_{男,吸} = 100 × 60 / 200 = 30

E_{男,不吸} = 100 × 140 / 200 = 70

E_{女,吸} = 100 × 60 / 200 = 30

E_{女,不吸} = 100 × 140 / 200 = 70

**检验统计量**：

χ² = (40-30)²/30 + (60-70)²/70 + (20-30)²/30 + (80-70)²/70
   = 3.33 + 1.43 + 3.33 + 1.43
   = 9.52

自由度 = (r-1)(c-1) = 1×1 = 1

χ²_{0.05}(1) = 3.84

χ² = 9.52 > 3.84，拒绝H₀，性别与吸烟有关。

#### 15.6.3 齐性检验

**问题**：多个总体的分布是否相同？

与独立性检验的计算方法相同，但问题设置不同：

- 独立性检验：一个样本，两个变量
- 齐性检验：多个样本，一个变量

---

### 15.7 Fisher精确检验

**问题**：2×2列联表，样本量小时的独立性检验。

**优点**：不依赖大样本近似，精确计算p值。

**方法**：固定边际，计算各种可能表格的概率。

**超几何分布**：

P(表格) = C(a+b,a) × C(c+d,c) / C(n, a+c)

**适用条件**：

- 样本量小（n < 20）
- 期望频数有小于5的格子

---

### 15.8 游程检验

**问题**：数据序列是否随机？

**游程**：连续相同符号的序列。

**例**：+ + + - - + + - - - +

游程：+++, --, ++, ---, +

共5个游程。

**检验**：游程数太多或太少都说明非随机。

**大样本近似**：

E(R) = 2n₁n₂/(n₁+n₂) + 1

Var(R) = 2n₁n₂(2n₁n₂-n₁-n₂) / [(n₁+n₂)²(n₁+n₂-1)]

Z = (R - E(R)) / √Var(R) 近似 N(0,1)

---

### 15.9 非参数检验速查

| 参数检验 | 对应的非参数检验 |
|----------|------------------|
| 单样本t检验 | 符号检验、Wilcoxon符号秩检验 |
| 两独立样本t检验 | Wilcoxon秩和检验（Mann-Whitney U） |
| 配对t检验 | Wilcoxon符号秩检验 |
| 单因素方差分析 | Kruskal-Wallis检验 |
| 双因素方差分析 | Friedman检验 |
| Pearson相关 | Spearman相关、Kendall τ |

---

### 本章小结

1. **非参数检验**：不假设分布形式，更稳健
2. **符号检验**：检验中位数，只用符号
3. **Wilcoxon符号秩**：改进符号检验，利用秩信息
4. **Wilcoxon秩和**：两独立样本比较（Mann-Whitney U）
5. **Kruskal-Wallis**：多组比较，方差分析的非参数版
6. **卡方检验**：拟合优度、独立性、齐性
7. **Fisher精确检验**：小样本2×2表
8. **游程检验**：检验随机性

---

*下一章：贝叶斯统计初步*

---

## 第16章 贝叶斯统计初步

### 16.1 频率派 vs 贝叶斯派

**频率派（Frequentist）**：

- 参数θ是固定但未知的常数
- 概率是长期频率
- 推断基于样本的抽样分布

**贝叶斯派（Bayesian）**：

- 参数θ是随机变量，有自己的分布
- 概率是信念程度
- 推断基于后验分布

| | 频率派 | 贝叶斯派 |
|--|--------|----------|
| 参数 | 固定常数 | 随机变量 |
| 概率含义 | 长期频率 | 主观信念 |
| 先验信息 | 不使用 | 显式使用 |
| 结论 | p值、置信区间 | 后验分布、可信区间 |

---

### 16.2 贝叶斯公式回顾

**贝叶斯公式**：

P(θ|X) = P(X|θ) × P(θ) / P(X)

读作：后验概率等于似然乘以先验，除以证据。

**术语**：

- P(θ)：**先验分布**，观测数据前对θ的信念
- P(X|θ)：**似然函数**，给定θ时观测到X的概率
- P(θ|X)：**后验分布**，观测数据后对θ的信念
- P(X)：**边际似然/证据**，归一化常数

**简化形式**：

P(θ|X) ∝ P(X|θ) × P(θ)

读作：后验正比于似然乘以先验。

---

### 16.3 贝叶斯推断的流程

1. **选择先验分布 P(θ)**：反映对参数的先验知识
2. **建立似然函数 P(X|θ)**：基于统计模型
3. **计算后验分布 P(θ|X)**：用贝叶斯公式更新
4. **从后验得出结论**：点估计、区间估计、预测

---

### 16.4 先验分布

#### 16.4.1 先验的类型

**无信息先验（Uninformative Prior）**：

对参数没有偏好，"让数据说话"。

例如：均匀分布 U(0, 1) 或 U(-∞, +∞)

**信息先验（Informative Prior）**：

基于领域知识设定。

例如：已知某参数通常在0.5附近，用N(0.5, 0.1²)

**共轭先验（Conjugate Prior）**：

使得后验分布与先验分布属于同一分布族。

计算方便，有解析解。

#### 16.4.2 常见共轭先验

| 似然 | 共轭先验 | 后验 |
|------|----------|------|
| 二项分布 | Beta | Beta |
| 泊松分布 | Gamma | Gamma |
| 正态（已知方差） | 正态 | 正态 |
| 正态（未知方差） | 逆Gamma | 逆Gamma |

---

### 16.5 Beta-二项模型

**问题**：抛硬币n次，正面出现x次。θ = P(正面) 是多少？

**模型**：

X | θ ~ Binomial(n, θ)

θ ~ Beta(α, β)

**先验**：

P(θ) = θ^{α-1} (1-θ)^{β-1} / B(α, β)

读作：θ服从参数为α和β的Beta分布。

**似然**：

P(X=x|θ) = C(n,x) × θˣ × (1-θ)^{n-x}

**后验**：

P(θ|X=x) ∝ θ^{x+α-1} × (1-θ)^{n-x+β-1}

即 θ|X ~ Beta(α + x, β + n - x)

读作：后验是参数更新后的Beta分布。

**直觉**：

- α可理解为"先验中正面次数"
- β可理解为"先验中反面次数"
- 观测后：正面次数变为α+x，反面次数变为β+n-x

**例**：

先验：θ ~ Beta(1, 1)（均匀分布，表示无信息）

观测：抛10次，7次正面

后验：θ|X ~ Beta(1+7, 1+3) = Beta(8, 4)

后验均值：E(θ|X) = 8/(8+4) = 0.667

后验众数：(8-1)/(8+4-2) = 7/10 = 0.7

---

### 16.6 贝叶斯点估计

**后验均值**：

θ̂ = E(θ|X) = ∫ θ × P(θ|X) dθ

**后验众数（MAP估计）**：

θ̂_MAP = argmax_θ P(θ|X)

**后验中位数**：

使P(θ < θ̂|X) = 0.5的值。

**比较**：

- 后验均值：最小化平方损失
- 后验众数：最大后验概率点
- 后验中位数：最小化绝对损失

---

### 16.7 可信区间

**定义**：给定后验分布，找到区间[a, b]使得：

P(a < θ < b | X) = 1 - α

称为**可信区间（Credible Interval）**。

**与置信区间的区别**：

| | 置信区间 | 可信区间 |
|--|----------|----------|
| 含义 | 重复抽样，95%区间包含真值 | 参数有95%概率在区间内 |
| 参数 | 固定常数 | 随机变量 |
| 解释 | 频率解释 | 概率解释 |

**可信区间更直观**：直接回答"参数在哪里"。

**常用可信区间**：

1. **等尾区间**：左右各剪去α/2的尾部
2. **最高后验密度区间（HPD）**：包含后验密度最高的点

---

### 16.8 贝叶斯假设检验

**方法1：比较后验概率**

计算P(H₀|X)和P(H₁|X)，选概率大的。

**方法2：贝叶斯因子**

BF = P(X|H₁) / P(X|H₀)

读作：数据在H₁下的边际似然除以在H₀下的边际似然。

**解释**：

| BF | 证据强度 |
|----|----------|
| 1-3 | 弱 |
| 3-10 | 中等 |
| 10-30 | 强 |
| >30 | 非常强 |

**方法3：检验参数是否在某区间内**

计算P(θ > 0|X)，如果>0.95，则认为θ显著为正。

---

### 16.9 先验的敏感性

**问题**：结论对先验选择有多敏感？

**建议**：

1. 尝试不同的先验，看后验是否变化大
2. 样本量大时，似然主导，先验影响小
3. 样本量小时，先验影响大，需谨慎选择

**经验法则**：

当n → ∞，后验 → 似然（数据说了算）

当n → 0，后验 → 先验（信念说了算）

---

### 16.10 贝叶斯方法的优缺点

**优点**：

1. 自然地纳入先验知识
2. 结论更直观（概率语言）
3. 可以做序贯更新（逐步加入数据）
4. 避免p值的误解问题
5. 自动的"奥卡姆剃刀"效应

**缺点**：

1. 先验选择有主观性
2. 计算复杂（常需MCMC）
3. 结果依赖先验（小样本时）

---

### 本章小结

1. **贝叶斯派**：参数是随机变量，有先验分布
2. **核心公式**：后验 ∝ 似然 × 先验
3. **共轭先验**：使后验与先验同族
4. **Beta-二项**：Beta(α,β) + x次成功 → Beta(α+x, β+n-x)
5. **贝叶斯点估计**：后验均值、众数、中位数
6. **可信区间**：参数有95%概率在区间内（更直观）
7. **贝叶斯因子**：比较两个假设的证据强度
8. **大样本**：后验由数据主导，先验影响小

---

*下一章：随机过程简介*

---

## 第17章 随机过程简介

### 17.1 什么是随机过程

**定义**：随机过程是一族随机变量{X(t), t ∈ T}。

- t通常表示时间
- T是指标集（参数空间）
- 对每个固定的t，X(t)是一个随机变量

**直觉**：随机变量描述一个时刻的不确定性；随机过程描述随时间演化的不确定性。

**例**：

- 股票价格随时间的变化
- 粒子的随机运动（布朗运动）
- 排队系统中的队列长度
- 信号中的噪声

---

### 17.2 随机过程的分类

#### 17.2.1 按时间参数分类

**离散时间过程**：T = {0, 1, 2, 3, ...}

例：每日股价、每小时温度

**连续时间过程**：T = [0, ∞) 或 T = ℝ

例：连续监测的温度、股票实时价格

#### 17.2.2 按状态空间分类

**离散状态**：X(t)取值于有限或可数集

例：排队人数（0, 1, 2, ...）

**连续状态**：X(t)取值于实数或区间

例：股票价格、温度

#### 17.2.3 常见类型

| 时间 | 状态 | 例子 |
|------|------|------|
| 离散 | 离散 | 随机游走、马尔可夫链 |
| 离散 | 连续 | ARMA时间序列 |
| 连续 | 离散 | 泊松过程、生灭过程 |
| 连续 | 连续 | 布朗运动、扩散过程 |

---

### 17.3 随机游走

**定义**：从原点出发，每步等概率向左或向右移动一步。

设Xₙ为n步后的位置：

Xₙ = Z₁ + Z₂ + ... + Zₙ

其中Zᵢ独立同分布，P(Zᵢ=1) = P(Zᵢ=-1) = 0.5。

**性质**：

1. E(Xₙ) = 0
2. Var(Xₙ) = n
3. 几乎必然返回原点（一维）
4. 高维（≥3维）不一定返回原点

**应用**：

- 股价的简单模型
- 赌徒输赢的累积
- 分子扩散

---

### 17.4 马尔可夫链

#### 17.4.1 定义

**马尔可夫性**："未来只依赖于现在，与过去无关"

P(Xₙ₊₁ = j | X₀, X₁, ..., Xₙ) = P(Xₙ₊₁ = j | Xₙ)

读作：给定当前状态，未来与过去条件独立。

**马尔可夫链**：满足马尔可夫性的离散时间离散状态随机过程。

#### 17.4.2 转移概率

**一步转移概率**：

pᵢⱼ = P(Xₙ₊₁ = j | Xₙ = i)

读作：从状态i一步转移到状态j的概率。

**转移矩阵**：

P = [pᵢⱼ]

性质：每行之和为1。

**例：天气模型**

状态：晴(1)、阴(2)、雨(3)

```
P = [0.7  0.2  0.1]    晴→晴0.7, 晴→阴0.2, 晴→雨0.1
    [0.3  0.4  0.3]    阴→晴0.3, 阴→阴0.4, 阴→雨0.3
    [0.2  0.3  0.5]    雨→晴0.2, 雨→阴0.3, 雨→雨0.5
```

#### 17.4.3 n步转移概率

**Chapman-Kolmogorov方程**：

P⁽ⁿ⁾ = Pⁿ

读作：n步转移矩阵等于一步转移矩阵的n次方。

**例**：今天是晴天，求后天是雨天的概率。

P² = P × P

P²₁₃ = 后天从晴到雨的概率

#### 17.4.4 平稳分布

**定义**：满足 πP = π 的分布π称为**平稳分布**。

读作：从平稳分布出发，经过一步转移后仍是平稳分布。

**存在性**：若马尔可夫链不可约且非周期，则存在唯一平稳分布。

**长期行为**：

lim_{n→∞} P⁽ⁿ⁾ᵢⱼ = πⱼ

读作：无论从哪个状态出发，长期后处于状态j的概率趋向于πⱼ。

---

### 17.5 泊松过程

#### 17.5.1 定义

**计数过程**：N(t)表示时间[0, t]内事件发生的次数。

**泊松过程**：满足以下条件的计数过程：

1. N(0) = 0
2. 独立增量：不相交区间上的事件数独立
3. 平稳增量：事件数只依赖于区间长度
4. P(N(h) = 1) = λh + o(h)
5. P(N(h) ≥ 2) = o(h)

**结果**：

N(t) ~ Poisson(λt)

读作：时间t内的事件数服从参数λt的泊松分布。

**λ的含义**：单位时间内事件的平均发生率。

#### 17.5.2 到达时间间隔

设Tₙ为第n-1次到第n次事件的时间间隔。

Tₙ ~ Exp(λ)，独立同分布

读作：相邻事件的时间间隔服从指数分布。

**无记忆性**：指数分布的无记忆性正好对应泊松过程的无记忆性。

#### 17.5.3 到达时间

设Sₙ为第n次事件发生的时间。

Sₙ = T₁ + T₂ + ... + Tₙ ~ Gamma(n, λ)

读作：第n次事件的发生时间服从Gamma分布。

#### 17.5.4 应用

- 电话呼入
- 顾客到达
- 放射性衰变
- 网站访问
- 保险索赔

---

### 17.6 布朗运动（简介）

**定义**：布朗运动（Wiener过程）{W(t), t ≥ 0}满足：

1. W(0) = 0
2. 独立增量
3. W(t) - W(s) ~ N(0, t-s)，对所有0 ≤ s < t
4. 路径连续

**性质**：

1. E(W(t)) = 0
2. Var(W(t)) = t
3. Cov(W(s), W(t)) = min(s, t)
4. 路径处处连续但处处不可微

**应用**：

- 股价模型（几何布朗运动）
- 物理学中的扩散
- 期权定价（Black-Scholes模型）

**几何布朗运动**：

dS = μS dt + σS dW

解为：S(t) = S(0) exp((μ - σ²/2)t + σW(t))

这是股票价格的经典模型。

---

### 17.7 随机过程与AI

**马尔可夫链在AI中的应用**：

1. **马尔可夫决策过程（MDP）**：强化学习的基础
2. **隐马尔可夫模型（HMM）**：语音识别、NLP
3. **马尔可夫链蒙特卡洛（MCMC）**：贝叶斯推断
4. **PageRank**：网页排名算法

**泊松过程在AI中的应用**：

1. 事件预测
2. 异常检测
3. 排队系统建模

---

### 本章小结

1. **随机过程**：一族随机变量{X(t), t ∈ T}
2. **分类**：按时间（离散/连续）和状态（离散/连续）
3. **随机游走**：每步等概率+1或-1
4. **马尔可夫链**：未来只依赖现在
5. **转移矩阵**：P = [pᵢⱼ]，n步转移P⁽ⁿ⁾ = Pⁿ
6. **平稳分布**：πP = π
7. **泊松过程**：N(t) ~ Poisson(λt)，间隔~Exp(λ)
8. **布朗运动**：连续路径，独立正态增量

---

*下一章：信息论基础*

---

## 第18章 信息论基础

### 18.1 信息的度量

**问题**：如何量化"信息量"？

**直觉**：

- 稀有事件包含更多信息（"太阳从西边升起"比"太阳从东边升起"信息量大）
- 独立事件的信息量应该相加

**Shannon的定义**：

事件A的信息量（自信息）：

I(A) = -log P(A)

读作：信息量等于概率的负对数。

**底数选择**：

- 底数为2：单位是比特（bit）
- 底数为e：单位是纳特（nat）
- 底数为10：单位是哈特利（hartley）

**例**：

- 抛硬币正面：I = -log₂(0.5) = 1 bit
- 抛骰子出6：I = -log₂(1/6) ≈ 2.58 bits
- 必然事件：I = -log₂(1) = 0 bits

---

### 18.2 熵

#### 18.2.1 定义

**熵（Entropy）**：随机变量不确定性的度量。

H(X) = -Σᵢ P(xᵢ) log P(xᵢ) = E[-log P(X)]

读作：熵等于各取值概率乘以其信息量的和。

**直觉**：熵是信息量的期望。

#### 18.2.2 性质

1. **非负性**：H(X) ≥ 0
2. **最大熵**：离散随机变量的熵在均匀分布时最大
3. **均匀分布熵**：H = log n（n是取值个数）

**例：二元熵函数**

X ~ Bernoulli(p)

H(p) = -p log p - (1-p) log(1-p)

- H(0) = H(1) = 0（确定性，无不确定性）
- H(0.5) = 1 bit（最大不确定性）

#### 18.2.3 联合熵与条件熵

**联合熵**：

H(X, Y) = -Σᵢⱼ P(xᵢ, yⱼ) log P(xᵢ, yⱼ)

读作：联合分布的熵。

**条件熵**：

H(Y|X) = -Σᵢⱼ P(xᵢ, yⱼ) log P(yⱼ|xᵢ)
       = H(X, Y) - H(X)

读作：已知X后Y的平均不确定性。

**链式法则**：

H(X, Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)

读作：联合熵等于边缘熵加条件熵。

---

### 18.3 互信息

**定义**：

I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)

读作：知道Y后，X的不确定性减少了多少。

**等价形式**：

I(X; Y) = H(X) + H(Y) - H(X, Y)

I(X; Y) = Σᵢⱼ P(xᵢ, yⱼ) log [P(xᵢ, yⱼ) / (P(xᵢ)P(yⱼ))]

**性质**：

1. I(X; Y) ≥ 0
2. I(X; Y) = 0 当且仅当 X, Y 独立
3. I(X; Y) = I(Y; X)（对称性）
4. I(X; X) = H(X)

**直觉**：互信息衡量两个变量的共享信息量，是相关性的一种度量。

---

### 18.4 KL散度

**Kullback-Leibler散度**（相对熵）：

D_KL(P || Q) = Σᵢ P(xᵢ) log [P(xᵢ) / Q(xᵢ)]

读作：分布P相对于分布Q的散度。

**性质**：

1. D_KL(P || Q) ≥ 0（Gibbs不等式）
2. D_KL(P || Q) = 0 当且仅当 P = Q
3. **不对称**：D_KL(P || Q) ≠ D_KL(Q || P)

**直觉**：用Q近似P时的信息损失。

**与互信息的关系**：

I(X; Y) = D_KL(P(X,Y) || P(X)P(Y))

读作：互信息是联合分布与独立分布的KL散度。

---

### 18.5 交叉熵

**定义**：

H(P, Q) = -Σᵢ P(xᵢ) log Q(xᵢ)

读作：用Q的编码方式对P分布的数据编码的平均长度。

**与KL散度的关系**：

H(P, Q) = H(P) + D_KL(P || Q)

读作：交叉熵等于真实熵加KL散度。

**在机器学习中的应用**：

分类问题的损失函数：

L = -Σᵢ yᵢ log ŷᵢ

其中y是真实标签（one-hot），ŷ是预测概率。

这就是交叉熵损失，最小化它等价于最小化KL散度。

---

### 18.6 信息论在AI中的应用

#### 18.6.1 特征选择

**互信息准则**：选择与目标变量互信息最大的特征。

I(X; Y) 越大，特征X对预测Y越有用。

#### 18.6.2 决策树

**信息增益**：分裂前后熵的减少量。

IG(Y, X) = H(Y) - H(Y|X)

选择信息增益最大的特征进行分裂。

#### 18.6.3 神经网络

**交叉熵损失**：分类任务的标准损失函数。

**信息瓶颈理论**：神经网络学习压缩输入，保留与输出相关的信息。

#### 18.6.4 变分推断

**ELBO（证据下界）**：

log P(X) ≥ E_q[log P(X,Z)] + H(q)
        = E_q[log P(X|Z)] - D_KL(q(Z) || P(Z))

最大化ELBO等价于最小化q与后验的KL散度。

#### 18.6.5 生成模型

**VAE**：使用KL散度正则化隐变量。

**GAN**：隐式最小化生成分布与真实分布的JS散度。

---

### 18.7 最大熵原理

**原理**：在满足已知约束的所有分布中，选择熵最大的分布。

**直觉**：不要假设你不知道的信息。

**应用**：

1. 只知道均值 → 指数分布
2. 知道均值和方差 → 正态分布
3. 知道范围 → 均匀分布

**最大熵模型**：

P(y|x) = (1/Z) exp(Σᵢ λᵢ fᵢ(x, y))

其中fᵢ是特征函数，λᵢ是参数。

这是逻辑回归和Softmax的理论基础。

---

### 18.8 数据压缩基础

**Shannon编码定理**：

平均编码长度的下界是熵。

L ≥ H(X)

读作：无损压缩的平均码长至少是熵。

**霍夫曼编码**：

一种接近熵的最优前缀编码。

**例**：

符号概率：A(0.5), B(0.25), C(0.125), D(0.125)

H = 1.75 bits

霍夫曼编码：A→0, B→10, C→110, D→111

平均长度：0.5×1 + 0.25×2 + 0.125×3 + 0.125×3 = 1.75 bits

达到了熵的下界！

---

### 本章小结

1. **自信息**：I(A) = -log P(A)
2. **熵**：H(X) = E[-log P(X)]，不确定性的度量
3. **条件熵**：H(Y|X) = H(X,Y) - H(X)
4. **互信息**：I(X;Y) = H(X) - H(X|Y)，共享信息量
5. **KL散度**：D_KL(P||Q)，分布差异的度量（不对称）
6. **交叉熵**：H(P,Q) = H(P) + D_KL(P||Q)
7. **应用**：特征选择、决策树、神经网络损失函数
8. **最大熵**：在约束下选择最不确定的分布

---

*下一章：蒙特卡洛方法*

---

## 第19章 蒙特卡洛方法

### 19.1 蒙特卡洛方法的思想

**核心思想**：用随机采样来近似计算。

**名称由来**：蒙特卡洛是摩纳哥的赌场，象征随机性。

**适用场景**：

1. 解析解难以求得
2. 高维积分
3. 复杂概率计算
4. 优化问题

**基本原理**：大数定律

E[f(X)] ≈ (1/n) Σᵢ f(Xᵢ)

读作：期望可以用样本均值近似，n越大越准确。

---

### 19.2 蒙特卡洛积分

**问题**：计算积分 I = ∫ f(x) dx

**方法1：简单采样**

设积分区间[a, b]，从U(a, b)中抽样X₁, ..., Xₙ。

I ≈ (b-a) × (1/n) Σᵢ f(Xᵢ)

读作：积分约等于区间长度乘以函数值的平均。

**例**：计算 ∫₀¹ x² dx

真实值：1/3 ≈ 0.333

蒙特卡洛：生成X₁,...,X₁₀₀₀ ~ U(0,1)

Î = (1/1000) Σ Xᵢ² ≈ 0.334

**方法2：重要性采样**

如果f(x)在某些区域很大，应该在那里多采样。

I = ∫ f(x) dx = ∫ [f(x)/g(x)] × g(x) dx = E_g[f(X)/g(X)]

选择g(x)使得f(x)/g(x)的方差小。

**例**：计算稀有事件概率

P(X > 10)，X ~ N(0,1)

直接采样：需要大量样本才能采到X > 10

重要性采样：从N(10, 1)采样，然后加权校正

---

### 19.3 蒙特卡洛估计的精度

**标准误**：

SE = σ / √n

读作：估计的标准误与样本量的平方根成反比。

**置信区间**：

Î ± z_{α/2} × SE

**关键点**：

- 精度∝1/√n，要提高一个数量级精度需要100倍样本
- 与维度无关！这是蒙特卡洛在高维问题中的优势

**方差缩减技术**：

1. **对偶变量法**：利用负相关
2. **控制变量法**：利用已知期望的辅助变量
3. **分层采样**：按区域分层采样
4. **重要性采样**：在重要区域多采样

---

### 19.4 马尔可夫链蒙特卡洛（MCMC）

**问题**：如何从复杂分布π(x)中采样？

**思想**：构造一个马尔可夫链，使其平稳分布是π(x)。

长期运行后，样本近似来自π(x)。

#### 19.4.1 Metropolis-Hastings算法

**步骤**：

1. 初始化x₀
2. 对t = 0, 1, 2, ...：
   - 从提议分布q(x'|xₜ)采样候选x'
   - 计算接受概率：α = min(1, [π(x')q(xₜ|x')] / [π(xₜ)q(x'|xₜ)])
   - 以概率α接受：xₜ₊₁ = x'，否则xₜ₊₁ = xₜ

**关键点**：

- 只需要知道π(x)的比例，不需要归一化常数
- 这正是贝叶斯推断中需要的！

**例：从N(0,1)采样**

提议分布：q(x'|x) = N(x, σ²)（随机游走）

接受概率：α = min(1, exp(-(x'²-x²)/2))

#### 19.4.2 Gibbs采样

**适用于**：多维分布，条件分布易于采样。

**步骤**：

对每个维度i：

xᵢ^(t+1) ~ P(xᵢ | x₁^(t+1), ..., xᵢ₋₁^(t+1), xᵢ₊₁^(t), ..., xₙ^(t))

读作：固定其他维度，从条件分布采样当前维度。

**例：二维正态分布**

(X, Y) ~ N₂(μ, Σ)

Gibbs采样：

X^(t+1) ~ P(X | Y = y^(t))

Y^(t+1) ~ P(Y | X = x^(t+1))

条件分布都是一维正态，容易采样。

#### 19.4.3 MCMC诊断

**燃烧期（Burn-in）**：丢弃初始样本，等待收敛。

**自相关**：相邻样本高度相关，有效样本量小于实际样本量。

**诊断方法**：

1. **轨迹图**：观察是否稳定
2. **自相关图**：检查相关性衰减
3. **Gelman-Rubin诊断**：比较多条链

**改进方法**：

- 间隔采样（thinning）：每隔k个样本取一个
- 多条链：从不同起点启动多条链

---

### 19.5 蒙特卡洛在AI中的应用

#### 19.5.1 贝叶斯神经网络

**问题**：对权重的不确定性建模。

**方法**：用MCMC从权重的后验分布采样。

预测时对多个权重采样取平均，得到不确定性估计。

#### 19.5.2 强化学习

**蒙特卡洛策略评估**：

用实际回报的平均估计价值函数。

V(s) ≈ (1/n) Σᵢ Gᵢ(s)

其中Gᵢ(s)是从状态s出发的第i次回报。

**蒙特卡洛树搜索（MCTS）**：

用于游戏AI（如AlphaGo）。

1. 选择：沿树向下选择节点
2. 扩展：添加新节点
3. 模拟：随机对局到终局
4. 回传：更新路径上的统计量

#### 19.5.3 变分推断

**重参数化技巧**：

将采样转化为确定性变换+噪声。

z = μ + σ × ε，ε ~ N(0, 1)

这样梯度可以回传，用于训练VAE。

#### 19.5.4 生成模型

**扩散模型**：

逐步添加噪声（正向过程），然后学习去噪（逆向过程）。

采样时从纯噪声开始，逐步去噪生成样本。

---

### 19.6 随机优化

#### 19.6.1 随机梯度下降（SGD）

**问题**：最小化 L(θ) = (1/n) Σᵢ Lᵢ(θ)

**全批量梯度下降**：

θ ← θ - η × (1/n) Σᵢ ∇Lᵢ(θ)

计算量大。

**随机梯度下降**：

θ ← θ - η × ∇Lᵢ(θ)

每次只用一个样本，梯度是无偏估计。

**小批量SGD**：

θ ← θ - η × (1/B) Σⱼ∈batch ∇Lⱼ(θ)

平衡计算效率和梯度方差。

#### 19.6.2 模拟退火

**问题**：找全局最优，避免局部最优。

**方法**：

1. 高温时接受较差解的概率高（探索）
2. 低温时只接受较好解（利用）

接受概率：

P(接受) = exp(-ΔE / T)

T逐渐降低（退火）。

---

### 本章小结

1. **蒙特卡洛**：用随机采样近似计算
2. **蒙特卡洛积分**：I ≈ (1/n) Σ f(Xᵢ)
3. **精度**：SE = σ/√n，与维度无关
4. **MCMC**：构造马尔可夫链从目标分布采样
5. **Metropolis-Hastings**：提议-接受框架
6. **Gibbs采样**：逐维度条件采样
7. **AI应用**：贝叶斯推断、强化学习、生成模型
8. **SGD**：随机梯度是全梯度的无偏估计

---

*下一章：概率图模型简介*

---

## 第20章 概率图模型简介

### 20.1 什么是概率图模型

**定义**：用图表示随机变量之间的概率关系。

**图的组成**：

- **节点**：随机变量
- **边**：变量之间的依赖关系

**两大类**：

| 类型 | 边 | 代表 |
|------|-----|------|
| 有向图 | 有向边（箭头） | 贝叶斯网络 |
| 无向图 | 无向边 | 马尔可夫随机场 |

**优势**：

1. 直观表达变量间的依赖
2. 利用图结构简化计算
3. 模块化建模
4. 便于推断和学习

---

### 20.2 贝叶斯网络

#### 20.2.1 定义

**贝叶斯网络**：有向无环图（DAG），节点是随机变量，边表示因果/依赖关系。

**联合分布分解**：

P(X₁, X₂, ..., Xₙ) = Π P(Xᵢ | Parents(Xᵢ))

读作：联合分布等于各节点条件概率的乘积。

每个变量只依赖于它的父节点。

#### 20.2.2 例子：学生成绩

变量：

- D：课程难度（难/易）
- I：学生智力（高/低）
- G：成绩（A/B/C）
- S：SAT分数（高/低）
- L：推荐信质量（好/差）

图结构：

D → G ← I → S
     ↓
     L

P(D, I, G, S, L) = P(D) × P(I) × P(G|D, I) × P(S|I) × P(L|G)

只需要存储5个条件概率表，而非完整的联合分布表。

#### 20.2.3 条件独立性

**局部马尔可夫性**：

给定父节点，每个节点条件独立于非后代节点。

**d-分离（d-separation）**：

判断两组变量是否条件独立的图论准则。

**三种基本结构**：

1. **链式** A → B → C：给定B，A与C独立
2. **分叉** A ← B → C：给定B，A与C独立
3. **v-结构** A → B ← C：不给定B，A与C独立；给定B，A与C不独立（解释away效应）

---

### 20.3 马尔可夫随机场

#### 20.3.1 定义

**马尔可夫随机场（MRF）**：无向图，边表示变量间的相互依赖。

**联合分布**：

P(X₁, ..., Xₙ) = (1/Z) Π ψ_c(X_c)

其中：

- ψ_c是势函数（potential function）
- X_c是团（clique）中的变量
- Z是归一化常数

读作：联合分布正比于各团势函数的乘积。

#### 20.3.2 条件独立性

**全局马尔可夫性**：

X_A ⊥ X_B | X_C，当且仅当C分离A和B（图论意义）。

**局部马尔可夫性**：

每个节点条件独立于其他节点，给定其邻居。

#### 20.3.3 例子：图像分割

像素构成网格，相邻像素倾向于同类。

势函数鼓励相邻像素取相同标签。

---

### 20.4 推断问题

#### 20.4.1 边缘推断

**问题**：计算P(Xᵢ)。

**挑战**：需要对其他所有变量求和/积分。

P(Xᵢ) = Σ_{X₋ᵢ} P(X₁, ..., Xₙ)

#### 20.4.2 条件推断

**问题**：给定证据E，计算P(Q|E)。

**例**：观察到成绩G=A，智力I的后验是什么？

P(I|G=A) = P(I, G=A) / P(G=A)

#### 20.4.3 MAP推断

**问题**：找最可能的配置。

X* = argmax_X P(X|E)

**例**：给定观测，最可能的隐状态序列是什么？（如HMM）

---

### 20.5 精确推断算法

#### 20.5.1 变量消除

**思想**：按某顺序逐个消除变量（求和）。

**复杂度**：取决于消除顺序，最坏指数级。

**例**：计算P(X₃)

P(X₃) = Σ_{X₁,X₂,X₄} P(X₁)P(X₂|X₁)P(X₃|X₂)P(X₄|X₃)

先消除X₄，再消除X₁，最后消除X₂。

#### 20.5.2 信念传播

**适用于**：树结构图。

**思想**：节点间传递"消息"，汇总后得边缘概率。

**Sum-Product算法**：计算边缘分布

**Max-Product算法**：计算MAP配置

#### 20.5.3 联结树算法

**思想**：将原图转化为树，在树上做信念传播。

适用于一般图，但树宽大时计算量大。

---

### 20.6 近似推断算法

#### 20.6.1 循环信念传播

在有环图上强行运行信念传播。

不保证收敛，但实践中常有效。

#### 20.6.2 变分推断

**思想**：用简单分布q(X)近似复杂后验p(X|E)。

最小化KL散度：D_KL(q || p)

等价于最大化ELBO。

**平均场近似**：假设q完全分解。

q(X) = Π q(Xᵢ)

#### 20.6.3 MCMC

用马尔可夫链蒙特卡洛从后验采样。

见第19章。

---

### 20.7 学习问题

#### 20.7.1 参数学习

**已知结构**：给定数据，估计条件概率表。

**完全数据**：用最大似然或贝叶斯估计。

**不完全数据**：用EM算法。

#### 20.7.2 结构学习

**未知结构**：从数据学习图结构。

**方法**：

1. **基于评分**：搜索最优结构，最大化评分（BIC等）
2. **基于约束**：用条件独立性检验发现边

---

### 20.8 常见概率图模型

#### 20.8.1 隐马尔可夫模型（HMM）

**结构**：隐状态序列 + 观测序列

Z₁ → Z₂ → Z₃ → ...
↓      ↓      ↓
X₁    X₂    X₃

**应用**：语音识别、基因序列分析

**三个基本问题**：

1. 评估：P(X|λ)（前向算法）
2. 解码：最可能的Z序列（Viterbi算法）
3. 学习：估计参数λ（Baum-Welch算法）

#### 20.8.2 条件随机场（CRF）

**判别模型**：直接建模P(Y|X)。

P(Y|X) = (1/Z) exp(Σ λⱼ fⱼ(Y, X))

**优势**：避免生成模型的独立性假设。

**应用**：序列标注、NER、词性标注

#### 20.8.3 深度生成模型

**VAE**：编码器-解码器 + 隐变量

**GAN**：生成器-判别器对抗

这些都可以用概率图模型的语言描述。

---

### 本章小结

1. **概率图模型**：用图表示变量间的概率关系
2. **贝叶斯网络**：有向图，P(X) = Π P(Xᵢ|Parents(Xᵢ))
3. **马尔可夫随机场**：无向图，P(X) ∝ Π ψ_c(X_c)
4. **推断**：边缘推断、条件推断、MAP推断
5. **精确推断**：变量消除、信念传播、联结树
6. **近似推断**：变分推断、MCMC
7. **学习**：参数学习、结构学习
8. **应用**：HMM、CRF、深度生成模型

---

*下一章：机器学习中的概率*

---

## 第21章 机器学习中的概率

### 21.1 概率视角的机器学习

**核心观点**：机器学习的本质是从数据中学习概率分布。

**三大任务**：

1. **监督学习**：学习P(Y|X)
2. **无监督学习**：学习P(X)
3. **强化学习**：学习P(R, S'|S, A)和最优策略

**概率带来什么？**

- 不确定性量化
- 模型可解释性
- 原则性的模型选择
- 自然处理缺失数据

---

### 21.2 最大似然估计与机器学习

**线性回归的概率解释**：

假设：Y = Xβ + ε，ε ~ N(0, σ²)

则：Y|X ~ N(Xβ, σ²)

**似然函数**：

L(β) = Π P(Yᵢ|Xᵢ, β) = Π (1/√(2πσ²)) exp(-(Yᵢ-Xᵢβ)²/(2σ²))

**对数似然**：

log L = -n/2 log(2πσ²) - (1/(2σ²)) Σ(Yᵢ-Xᵢβ)²

**最大化对数似然 ⟺ 最小化均方误差**

这就是为什么最小二乘回归是"对的"！

---

### 21.3 正则化的贝叶斯解释

#### 21.3.1 先验与正则化

**MAP估计**：

θ_MAP = argmax_θ P(θ|D) = argmax_θ P(D|θ)P(θ)

取对数：

θ_MAP = argmax_θ [log P(D|θ) + log P(θ)]

**正则化 = 先验**：

| 先验分布 | 对应正则化 |
|----------|------------|
| N(0, τ²) | L2正则化（岭回归） |
| Laplace(0, b) | L1正则化（Lasso） |
| 均匀分布 | 无正则化（MLE） |

#### 21.3.2 L2正则化 = 高斯先验

设 β ~ N(0, τ²I)

MAP估计：

β_MAP = argmax_β [-Σ(Yᵢ-Xᵢβ)²/(2σ²) - β'β/(2τ²)]
      = argmin_β [Σ(Yᵢ-Xᵢβ)² + λβ'β]

其中 λ = σ²/τ²。

这就是岭回归！

#### 21.3.3 L1正则化 = Laplace先验

设 βⱼ ~ Laplace(0, b)，P(β) ∝ exp(-|β|/b)

MAP估计导致Lasso：

β_MAP = argmin_β [Σ(Yᵢ-Xᵢβ)² + λΣ|βⱼ|]

Laplace先验在0处有尖峰，促使系数变为精确的0（稀疏性）。

---

### 21.4 分类的概率模型

#### 21.4.1 逻辑回归

**模型**：

P(Y=1|X) = σ(β'X) = 1/(1 + exp(-β'X))

σ是sigmoid函数。

**似然**：

L(β) = Π P(Yᵢ|Xᵢ)^Yᵢ × (1-P(Yᵢ|Xᵢ))^(1-Yᵢ)

**对数似然 = 负交叉熵**：

log L = Σ[Yᵢ log P(Yᵢ|Xᵢ) + (1-Yᵢ) log(1-P(Yᵢ|Xᵢ))]

最大化似然等价于最小化交叉熵损失。

#### 21.4.2 Softmax回归

**多分类**：

P(Y=k|X) = exp(βₖ'X) / Σⱼ exp(βⱼ'X)

这是最大熵模型的特例。

#### 21.4.3 朴素贝叶斯

**模型**：

P(Y|X) ∝ P(Y) × P(X|Y)

**朴素假设**：给定Y，特征条件独立。

P(X|Y) = Π P(Xⱼ|Y)

**优点**：

- 训练快（只需估计边缘分布）
- 适合高维稀疏数据（如文本）
- 对小样本表现好

**缺点**：

- 独立性假设过强
- 概率估计不准确（但分类常常有效）

---

### 21.5 生成模型 vs 判别模型

| | 生成模型 | 判别模型 |
|--|----------|----------|
| 建模 | P(X, Y) = P(X|Y)P(Y) | P(Y|X) |
| 例子 | 朴素贝叶斯、GMM、HMM | 逻辑回归、SVM、神经网络 |
| 优点 | 可生成数据、处理缺失值 | 通常分类更准确 |
| 缺点 | 需要更强假设 | 不能生成数据 |

**何时用生成模型？**

- 需要生成新样本
- 有大量未标注数据
- 需要处理缺失特征

---

### 21.6 不确定性量化

#### 21.6.1 两种不确定性

**认知不确定性（Epistemic Uncertainty）**：

模型参数的不确定性，可以通过更多数据减少。

**任意不确定性（Aleatoric Uncertainty）**：

数据本身的噪声，不能通过更多数据减少。

#### 21.6.2 贝叶斯神经网络

**思想**：对权重赋予先验分布，计算后验。

P(W|D) ∝ P(D|W) × P(W)

**预测时的不确定性**：

P(Y*|X*, D) = ∫ P(Y*|X*, W) P(W|D) dW

这个积分通常用MCMC或变分推断近似。

#### 21.6.3 MC Dropout

**简单方法**：训练时用Dropout，预测时也开启Dropout。

多次前向传播，得到预测分布。

```
预测分布 ≈ 多次Dropout前向传播的结果分布
```

这近似于一种变分推断。

---

### 21.7 期望最大化（EM）算法

**问题**：存在隐变量Z时，最大似然估计。

log P(X|θ) = log Σ_Z P(X, Z|θ)

直接优化困难（log里有求和）。

**EM算法**：

1. **E步**：计算隐变量的后验分布

   Q(Z) = P(Z|X, θ^(t))

2. **M步**：最大化期望完全对数似然

   θ^(t+1) = argmax_θ E_Q[log P(X, Z|θ)]

**保证**：每次迭代似然不减。

**应用**：

- 高斯混合模型（GMM）
- 隐马尔可夫模型（HMM）
- 缺失数据处理

---

### 21.8 模型选择与评估

#### 21.8.1 贝叶斯模型选择

**模型证据**：

P(D|M) = ∫ P(D|θ, M) P(θ|M) dθ

也叫边际似然。

**贝叶斯因子**：

BF = P(D|M₁) / P(D|M₂)

**Occam剃刀**：复杂模型的先验空间更大，证据自动被稀释。

#### 21.8.2 信息准则

**AIC**：-2 log L + 2k（k是参数数）

**BIC**：-2 log L + k log n（更惩罚复杂模型）

**DIC**：贝叶斯版本，用后验期望

选择准则值最小的模型。

---

### 本章小结

1. **概率视角**：机器学习是学习概率分布
2. **MLE**：最大似然 ⟺ 最小化损失
3. **正则化**：L2对应高斯先验，L1对应Laplace先验
4. **逻辑回归**：P(Y=1|X) = σ(β'X)，交叉熵损失
5. **生成 vs 判别**：建模P(X,Y) vs P(Y|X)
6. **不确定性**：认知不确定性 vs 任意不确定性
7. **EM算法**：E步估计隐变量，M步更新参数
8. **模型选择**：贝叶斯因子、AIC、BIC

---

*下一章：常见分布速查表*

---

## 第22章 常见分布速查表

### 22.1 离散分布

#### 伯努利分布 Bernoulli(p)

| 项目 | 内容 |
|------|------|
| 参数 | p ∈ (0, 1) |
| 取值 | X ∈ {0, 1} |
| PMF | P(X=1) = p, P(X=0) = 1-p |
| 期望 | E(X) = p |
| 方差 | Var(X) = p(1-p) |
| 应用 | 单次试验成功/失败 |

---

#### 二项分布 Binomial(n, p)

| 项目 | 内容 |
|------|------|
| 参数 | n ∈ ℕ, p ∈ (0, 1) |
| 取值 | X ∈ {0, 1, ..., n} |
| PMF | P(X=k) = C(n,k) pᵏ (1-p)ⁿ⁻ᵏ |
| 期望 | E(X) = np |
| 方差 | Var(X) = np(1-p) |
| 应用 | n次独立试验中成功次数 |

---

#### 泊松分布 Poisson(λ)

| 项目 | 内容 |
|------|------|
| 参数 | λ > 0 |
| 取值 | X ∈ {0, 1, 2, ...} |
| PMF | P(X=k) = λᵏ e⁻λ / k! |
| 期望 | E(X) = λ |
| 方差 | Var(X) = λ |
| 应用 | 稀有事件计数、到达次数 |

**与二项分布的关系**：n大p小时，Binomial(n, p) ≈ Poisson(np)

---

#### 几何分布 Geometric(p)

| 项目 | 内容 |
|------|------|
| 参数 | p ∈ (0, 1) |
| 取值 | X ∈ {1, 2, 3, ...} |
| PMF | P(X=k) = (1-p)ᵏ⁻¹ p |
| 期望 | E(X) = 1/p |
| 方差 | Var(X) = (1-p)/p² |
| 应用 | 首次成功所需试验次数 |

**无记忆性**：P(X > m+n | X > m) = P(X > n)

---

#### 负二项分布 NegBin(r, p)

| 项目 | 内容 |
|------|------|
| 参数 | r ∈ ℕ, p ∈ (0, 1) |
| 取值 | X ∈ {r, r+1, r+2, ...} |
| PMF | P(X=k) = C(k-1, r-1) pʳ (1-p)ᵏ⁻ʳ |
| 期望 | E(X) = r/p |
| 方差 | Var(X) = r(1-p)/p² |
| 应用 | 第r次成功所需试验次数 |

---

#### 超几何分布 Hypergeom(N, K, n)

| 项目 | 内容 |
|------|------|
| 参数 | N总体, K成功数, n抽取数 |
| 取值 | X ∈ {max(0,n-N+K), ..., min(n,K)} |
| PMF | P(X=k) = C(K,k)C(N-K,n-k) / C(N,n) |
| 期望 | E(X) = nK/N |
| 方差 | Var(X) = nK(N-K)(N-n) / [N²(N-1)] |
| 应用 | 无放回抽样中的成功数 |

---

### 22.2 连续分布

#### 均匀分布 U(a, b)

| 项目 | 内容 |
|------|------|
| 参数 | a < b |
| 取值 | X ∈ [a, b] |
| PDF | f(x) = 1/(b-a), x ∈ [a,b] |
| CDF | F(x) = (x-a)/(b-a) |
| 期望 | E(X) = (a+b)/2 |
| 方差 | Var(X) = (b-a)²/12 |

---

#### 指数分布 Exp(λ)

| 项目 | 内容 |
|------|------|
| 参数 | λ > 0（率参数） |
| 取值 | X ∈ [0, ∞) |
| PDF | f(x) = λ e⁻λˣ |
| CDF | F(x) = 1 - e⁻λˣ |
| 期望 | E(X) = 1/λ |
| 方差 | Var(X) = 1/λ² |

**无记忆性**：P(X > s+t | X > s) = P(X > t)

---

#### 正态分布 N(μ, σ²)

| 项目 | 内容 |
|------|------|
| 参数 | μ ∈ ℝ, σ > 0 |
| 取值 | X ∈ ℝ |
| PDF | f(x) = (1/√(2πσ²)) exp(-(x-μ)²/(2σ²)) |
| 期望 | E(X) = μ |
| 方差 | Var(X) = σ² |

**标准化**：Z = (X - μ)/σ ~ N(0, 1)

**68-95-99.7法则**：μ±σ含68%，μ±2σ含95%，μ±3σ含99.7%

---

#### Gamma分布 Gamma(α, β)

| 项目 | 内容 |
|------|------|
| 参数 | α > 0（形状），β > 0（率） |
| 取值 | X ∈ [0, ∞) |
| PDF | f(x) = βᵅ xᵅ⁻¹ e⁻βˣ / Γ(α) |
| 期望 | E(X) = α/β |
| 方差 | Var(X) = α/β² |

**特例**：Gamma(1, λ) = Exp(λ)，Gamma(n/2, 1/2) = χ²(n)

---

#### Beta分布 Beta(α, β)

| 项目 | 内容 |
|------|------|
| 参数 | α > 0, β > 0 |
| 取值 | X ∈ [0, 1] |
| PDF | f(x) = xᵅ⁻¹(1-x)ᵝ⁻¹ / B(α,β) |
| 期望 | E(X) = α/(α+β) |
| 方差 | Var(X) = αβ/[(α+β)²(α+β+1)] |

**应用**：概率的先验分布，与二项分布共轭

---

### 22.3 抽样分布

#### χ²分布 χ²(n)

| 项目 | 内容 |
|------|------|
| 参数 | n > 0（自由度） |
| 定义 | Z₁² + Z₂² + ... + Zₙ²，Zᵢ ~ N(0,1) |
| 期望 | E(X) = n |
| 方差 | Var(X) = 2n |
| 应用 | 方差检验、拟合优度检验 |

---

#### t分布 t(n)

| 项目 | 内容 |
|------|------|
| 参数 | n > 0（自由度） |
| 定义 | Z / √(V/n)，Z~N(0,1)，V~χ²(n) |
| 期望 | E(X) = 0（n > 1） |
| 方差 | Var(X) = n/(n-2)（n > 2） |
| 应用 | 小样本均值检验 |

**性质**：n→∞时，t(n)→N(0,1)

---

#### F分布 F(m, n)

| 项目 | 内容 |
|------|------|
| 参数 | m, n > 0（自由度） |
| 定义 | (U/m) / (V/n)，U~χ²(m)，V~χ²(n) |
| 期望 | E(X) = n/(n-2)（n > 2） |
| 应用 | 方差比检验、方差分析 |

**性质**：F(m, n) = 1/F(n, m)

---

### 22.4 分布关系图

```
伯努利 ──n次──→ 二项
   │              │
   │ n大p小       │ n大p小
   ↓              ↓
几何 ──r次──→ 负二项 ←── 泊松
                          │
                          │ 连续化
                          ↓
指数 ──n个和──→ Gamma ←── χ²
                   │
                   │ β/(α+β)
                   ↓
均匀 ←── 特例 ── Beta
```

正态分布的衍生：

```
N(0,1) ──平方和──→ χ²(n)
  │                  │
  │ Z/√(V/n)        │ 比值
  ↓                  ↓
t(n) ←────────────── F(m,n)
```

---

### 本章小结

本章汇总了概率统计中最常用的分布，包括：

- **离散**：伯努利、二项、泊松、几何、负二项、超几何
- **连续**：均匀、指数、正态、Gamma、Beta
- **抽样**：χ²、t、F

熟记这些分布的参数、期望、方差和应用场景。

---

*下一章：常用公式速查*

---

## 第23章 常用公式速查

### 23.1 概率基础公式

**加法公式**：
P(A∪B) = P(A) + P(B) - P(A∩B)

**条件概率**：
P(A|B) = P(A∩B) / P(B)

**乘法公式**：
P(A∩B) = P(A) × P(B|A) = P(B) × P(A|B)

**全概率公式**：
P(A) = Σᵢ P(A|Bᵢ) × P(Bᵢ)

**贝叶斯公式**：
P(Bⱼ|A) = P(A|Bⱼ)P(Bⱼ) / Σᵢ P(A|Bᵢ)P(Bᵢ)

**独立性**：
P(A∩B) = P(A) × P(B)

---

### 23.2 期望与方差公式

**期望**：
- 离散：E(X) = Σ xᵢ P(X=xᵢ)
- 连续：E(X) = ∫ x f(x) dx

**期望的性质**：
- E(aX + b) = aE(X) + b
- E(X + Y) = E(X) + E(Y)
- E(XY) = E(X)E(Y)（若独立）

**方差**：
Var(X) = E[(X - μ)²] = E(X²) - [E(X)]²

**方差的性质**：
- Var(aX + b) = a²Var(X)
- Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y)
- Var(X + Y) = Var(X) + Var(Y)（若独立）

**协方差**：
Cov(X, Y) = E[(X-μₓ)(Y-μᵧ)] = E(XY) - E(X)E(Y)

**相关系数**：
ρ = Cov(X, Y) / (σₓ × σᵧ)

---

### 23.3 常见分布的期望和方差

| 分布 | E(X) | Var(X) |
|------|------|--------|
| Bernoulli(p) | p | p(1-p) |
| Binomial(n, p) | np | np(1-p) |
| Poisson(λ) | λ | λ |
| Geometric(p) | 1/p | (1-p)/p² |
| U(a, b) | (a+b)/2 | (b-a)²/12 |
| Exp(λ) | 1/λ | 1/λ² |
| N(μ, σ²) | μ | σ² |
| Gamma(α, β) | α/β | α/β² |
| Beta(α, β) | α/(α+β) | αβ/[(α+β)²(α+β+1)] |
| χ²(n) | n | 2n |
| t(n) | 0 | n/(n-2) |

---

### 23.4 抽样分布公式

**样本均值**：
X̄ = (1/n) Σ Xᵢ

**样本方差**：
S² = (1/(n-1)) Σ(Xᵢ - X̄)²

**样本均值的期望和方差**：
- E(X̄) = μ
- Var(X̄) = σ²/n

**正态总体的抽样分布**：

X̄ ~ N(μ, σ²/n)

(X̄ - μ) / (σ/√n) ~ N(0, 1)

(X̄ - μ) / (S/√n) ~ t(n-1)

(n-1)S² / σ² ~ χ²(n-1)

---

### 23.5 置信区间公式

**均值（σ已知）**：
X̄ ± z_{α/2} × σ/√n

**均值（σ未知）**：
X̄ ± t_{α/2}(n-1) × S/√n

**方差**：
[(n-1)S² / χ²_{α/2}(n-1), (n-1)S² / χ²_{1-α/2}(n-1)]

**比例**：
p̂ ± z_{α/2} × √(p̂(1-p̂)/n)

**常用z值**：
- 90%置信：z_{0.05} = 1.645
- 95%置信：z_{0.025} = 1.96
- 99%置信：z_{0.005} = 2.576

---

### 23.6 假设检验统计量

**单样本均值（σ已知）**：
Z = (X̄ - μ₀) / (σ/√n)

**单样本均值（σ未知）**：
T = (X̄ - μ₀) / (S/√n) ~ t(n-1)

**单样本方差**：
χ² = (n-1)S² / σ₀² ~ χ²(n-1)

**两样本均值（方差相等）**：
T = (X̄₁ - X̄₂) / (Sₚ√(1/n₁ + 1/n₂)) ~ t(n₁+n₂-2)

其中 Sₚ² = [(n₁-1)S₁² + (n₂-1)S₂²] / (n₁+n₂-2)

**两样本方差**：
F = S₁² / S₂² ~ F(n₁-1, n₂-1)

---

### 23.7 回归分析公式

**一元线性回归**：
Ŷ = β̂₀ + β̂₁X

**斜率估计**：
β̂₁ = Σ(Xᵢ-X̄)(Yᵢ-Ȳ) / Σ(Xᵢ-X̄)² = Sxy / Sxx

**截距估计**：
β̂₀ = Ȳ - β̂₁X̄

**决定系数**：
R² = SSR/SST = 1 - SSE/SST

其中：
- SST = Σ(Yᵢ - Ȳ)²
- SSR = Σ(Ŷᵢ - Ȳ)²
- SSE = Σ(Yᵢ - Ŷᵢ)²

**斜率标准误**：
SE(β̂₁) = √(MSE / Sxx)

---

### 23.8 方差分析公式

**单因素方差分析**：

SST = SSA + SSE

- SST = Σᵢⱼ(Xᵢⱼ - X̄)²
- SSA = Σᵢ nᵢ(X̄ᵢ - X̄)²
- SSE = Σᵢⱼ(Xᵢⱼ - X̄ᵢ)²

**均方**：
- MSA = SSA / (k-1)
- MSE = SSE / (n-k)

**F统计量**：
F = MSA / MSE ~ F(k-1, n-k)

---

### 23.9 信息论公式

**熵**：
H(X) = -Σ P(xᵢ) log P(xᵢ)

**条件熵**：
H(Y|X) = H(X,Y) - H(X)

**互信息**：
I(X;Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y)

**KL散度**：
D_KL(P||Q) = Σ P(x) log[P(x)/Q(x)]

**交叉熵**：
H(P,Q) = -Σ P(x) log Q(x) = H(P) + D_KL(P||Q)

---

### 23.10 贝叶斯公式

**后验 ∝ 似然 × 先验**：
P(θ|X) ∝ P(X|θ) × P(θ)

**Beta-二项共轭**：
先验 Beta(α, β) + 数据(k成功, n-k失败)
→ 后验 Beta(α+k, β+n-k)

**正态-正态共轭**（已知方差σ²，先验N(μ₀, τ²)）：

后验均值 = (μ₀/τ² + nX̄/σ²) / (1/τ² + n/σ²)

---

### 本章小结

本章汇总了概率统计中最常用的公式，建议：

1. 理解公式背后的含义，而非死记
2. 通过例题熟练应用
3. 注意公式的适用条件

---

*下一章：统计表与临界值*

---

## 第24章 统计表与临界值

### 24.1 标准正态分布表

**常用临界值 z_α**：

| α | 0.10 | 0.05 | 0.025 | 0.01 | 0.005 |
|---|------|------|-------|------|-------|
| z_α | 1.282 | 1.645 | 1.960 | 2.326 | 2.576 |

**双侧检验临界值 z_{α/2}**：

| 置信度 | α | z_{α/2} |
|--------|---|---------|
| 90% | 0.10 | 1.645 |
| 95% | 0.05 | 1.960 |
| 99% | 0.01 | 2.576 |

**常用概率值**：

| z | P(Z ≤ z) |
|---|----------|
| -3 | 0.0013 |
| -2 | 0.0228 |
| -1 | 0.1587 |
| 0 | 0.5000 |
| 1 | 0.8413 |
| 2 | 0.9772 |
| 3 | 0.9987 |

---

### 24.2 t分布临界值

**t_{α}(n) 的部分取值**：

| n | t_{0.10} | t_{0.05} | t_{0.025} | t_{0.01} | t_{0.005} |
|---|----------|----------|-----------|----------|-----------|
| 1 | 3.078 | 6.314 | 12.706 | 31.821 | 63.657 |
| 5 | 1.476 | 2.015 | 2.571 | 3.365 | 4.032 |
| 10 | 1.372 | 1.812 | 2.228 | 2.764 | 3.169 |
| 20 | 1.325 | 1.725 | 2.086 | 2.528 | 2.845 |
| 30 | 1.310 | 1.697 | 2.042 | 2.457 | 2.750 |
| ∞ | 1.282 | 1.645 | 1.960 | 2.326 | 2.576 |

**使用说明**：

- 单侧检验查t_α
- 双侧检验查t_{α/2}
- n→∞时，t分布趋近于标准正态

---

### 24.3 χ²分布临界值

**χ²_{α}(n) 的部分取值**：

| n | χ²_{0.995} | χ²_{0.99} | χ²_{0.05} | χ²_{0.01} | χ²_{0.005} |
|---|------------|-----------|-----------|-----------|------------|
| 1 | 0.000 | 0.000 | 3.841 | 6.635 | 7.879 |
| 5 | 0.412 | 0.554 | 11.070 | 15.086 | 16.750 |
| 10 | 2.156 | 2.558 | 18.307 | 23.209 | 25.188 |
| 20 | 7.434 | 8.260 | 31.410 | 37.566 | 39.997 |
| 30 | 13.787 | 14.953 | 43.773 | 50.892 | 53.672 |

**使用说明**：

- 方差检验的上侧临界值用χ²_α
- 方差检验的下侧临界值用χ²_{1-α}

---

### 24.4 F分布临界值

**F_{0.05}(m, n) 的部分取值**：

| m\n | 1 | 5 | 10 | 20 | ∞ |
|-----|------|------|------|------|------|
| 1 | 161.4 | 230.2 | 241.9 | 248.0 | 254.3 |
| 5 | 6.61 | 5.05 | 4.74 | 4.56 | 4.36 |
| 10 | 4.96 | 3.33 | 2.98 | 2.77 | 2.54 |
| 20 | 4.35 | 2.71 | 2.35 | 2.12 | 1.84 |
| ∞ | 3.84 | 2.21 | 1.83 | 1.57 | 1.00 |

**使用说明**：

- 方差分析、方差比检验使用
- F_{α}(m, n) = 1 / F_{1-α}(n, m)

---

### 24.5 常用临界值速记

**95%置信水平**（最常用）：

| 检验类型 | 临界值 |
|----------|--------|
| Z检验 | ±1.96 |
| t检验(10) | ±2.228 |
| t检验(20) | ±2.086 |
| t检验(30) | ±2.042 |
| χ²检验(1) | 3.84 |
| χ²检验(5) | 11.07 |
| χ²检验(10) | 18.31 |

**记忆技巧**：

- z_{0.025} ≈ 2（精确1.96）
- t分布自由度越大越接近2
- χ²分布的期望等于自由度

---

### 24.6 p值与临界值对照

**Z检验**：

| p值 | 双侧|Z| | 单侧Z |
|-----|---------|--------|
| 0.10 | 1.645 | 1.282 |
| 0.05 | 1.960 | 1.645 |
| 0.01 | 2.576 | 2.326 |
| 0.001 | 3.291 | 3.090 |

**解读**：

- p < 0.05：*
- p < 0.01：**
- p < 0.001：***

---

### 本章小结

本章提供了常用统计分布的临界值表。实际应用中：

1. 建议使用统计软件计算精确p值
2. 表格用于快速估计和手算
3. 注意区分单侧和双侧检验

---

*下一章：学习路径建议*

---

## 第25章 学习路径建议

### 25.1 概率统计学习路线

**第一阶段：概率基础**（1-6章）

1. 随机事件与概率
2. 条件概率与贝叶斯
3. 随机变量与分布
4. 常见分布（离散+连续）
5. 期望、方差、协方差
6. 多维随机变量

**第二阶段：大样本理论**（7-8章）

7. 大数定律
8. 中心极限定理

**第三阶段：统计推断**（9-15章）

9. 抽样与统计量
10. 参数估计
11. 假设检验
12. 方差分析
13. 回归分析
14. 相关分析
15. 非参数检验

**第四阶段：进阶主题**（16-21章）

16. 贝叶斯统计
17. 随机过程
18. 信息论
19. 蒙特卡洛方法
20. 概率图模型
21. 机器学习中的概率

---

### 25.2 学习建议

**对于初学者**：

1. 先建立直觉，再学公式
2. 多做例题，理解应用场景
3. 画图辅助理解（分布形状、概率含义）
4. 不要死记公式，理解推导过程

**对于工程师/数据科学家**：

1. 重点掌握假设检验、回归、贝叶斯
2. 理解p值的正确解读
3. 学会用软件（Python/R）做统计分析
4. 关注机器学习中的概率应用

**对于研究人员**：

1. 深入理解理论基础
2. 掌握证明技巧
3. 学习高级主题（随机过程、概率图模型）
4. 阅读原始论文

---

### 25.3 常见误区

**误区1**：概率=频率

纠正：概率是数学模型，频率是实验观察。

**误区2**：p值是假设为真的概率

纠正：p值是在H₀为真时，观察到当前或更极端结果的概率。

**误区3**：相关意味着因果

纠正：相关只说明存在关联，不能推断因果。

**误区4**：95%置信区间意味着参数有95%概率在区间内

纠正：是重复抽样时，95%的区间会包含真值。

**误区5**：样本量越大越好

纠正：样本量应根据效应量、功效和成本综合考虑。

---

### 25.4 推荐资源

**教材**：

1. 《概率论与数理统计》- 陈希孺
2. 《统计学习方法》- 李航
3. "Pattern Recognition and Machine Learning" - Bishop
4. "The Elements of Statistical Learning" - Hastie等

**在线课程**：

1. MIT OpenCourseWare: Probability and Statistics
2. Stanford CS229: Machine Learning
3. Coursera: Bayesian Statistics

**工具**：

1. Python: numpy, scipy, statsmodels, scikit-learn
2. R: base R, ggplot2, caret
3. 可视化: matplotlib, seaborn, plotly

---

### 25.5 本资料的使用建议

1. **顺序学习**：按章节顺序，前后有依赖
2. **重点标记**：用"读作"理解公式含义
3. **动手练习**：每章后尝试用代码实现
4. **反复复习**：分布速查表和公式速查常看
5. **联系实际**：思考概念在实际问题中的应用

---

### 结语

概率统计是一门需要时间沉淀的学科。

不要追求一次性掌握所有内容。

遇到困难时，回到直觉和例子。

理论联系实际，在应用中深化理解。

祝学习顺利！

---

**全书完**
